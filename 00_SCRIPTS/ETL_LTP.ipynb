{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544ef674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mapeamento de pastas e importação de tabelas concluído!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from functions import *\n",
    "from FUNCTIONS_LTP import *\n",
    "import locale\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import range_boundaries\n",
    "import tempfile\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'Portuguese_Brazil.1252')  # Para Windows\n",
    "timer = Temporizador()\n",
    "timer.iniciar()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Aumenta o limite de largura da coluna para exibição\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Detecta se o script está sendo executado de um .py ou de um notebook\n",
    "try:\n",
    "    caminho_base = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # __file__ não existe em Jupyter ou ambiente interativo\n",
    "    caminho_base = Path.cwd()\n",
    "    \n",
    "pasta_input = caminho_base.parent / '01_INPUT'\n",
    "pasta_output = caminho_base.parent / '02_OUTPUT'\n",
    "pasta_painel = caminho_base.parent / '03_EXCEL'\n",
    "\n",
    "# Carregar bd_mat_param uma vez\n",
    "bd_mat_param = pd.read_excel(\n",
    "    pasta_input / 'matriz_parametros.xlsx',\n",
    "    sheet_name='matriz_parametros',\n",
    "    engine='calamine',\n",
    "    dtype={'produto': str}\n",
    ")\n",
    "bd_mat_param['produto'] = bd_mat_param['produto'].astype(str).str.strip().str.upper()\n",
    "produtos_param = set(bd_mat_param['produto'])\n",
    "colunas_manter = [\n",
    "    'produto', 'unidade_fat', 'unidade_prod', 'tipo_abast', 'prioridade'\n",
    "]\n",
    "\n",
    "bd_mat_param = bd_mat_param[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Carregar base_dados_produtos\n",
    "bd_prod = pd.read_excel(\n",
    "    pasta_input / 'base_dados_produtos.xlsx',\n",
    "    sheet_name='base_dados_produtos',\n",
    "    engine='calamine',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "bd_prod['cod_produto'] = bd_prod['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_prod['cod_produto'] = bd_prod['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_prod = bd_prod[bd_prod['cod_produto'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "if 'mes_ref' in bd_prod.columns:\n",
    "    bd_prod['mes_ref'] = pd.to_datetime(bd_prod['mes_ref'], errors='coerce', dayfirst=True)\n",
    "\n",
    "colunas_manter = ['mes_ref', 'empresa', 'cod_produto', 'descricao', 'linha_prod', 'familia_prod', 'tipo_produto', 'curva_abc', 'curva_123', 'estoq_seg_pcs', 'estoq_seg_kg', 'estoq_inicial_pcs', 'estoq_inicial_kg', 'carteira_arraste_mes_anterior', 'carteira_mes_atual', 'previsao_pcs', 'saldo_previsao_pcs', 'peso_produto_kg', 'estoq_transf_pcs', 'lote_econ', 'qtd_emb', 'lote_min']\n",
    "bd_prod = bd_prod[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Carregar estruturas\n",
    "colunas_bd_estruturas = ['mes_ref', 'empresa', 'cod_prod_acabado', 'cod_insumo', 'descricao', 'qtd_utilizada_pcs']\n",
    "bd_estruturas = pd.read_excel(\n",
    "    pasta_input / 'estruturas.xlsx',\n",
    "    sheet_name='estruturas',\n",
    "    engine='calamine',\n",
    "    dtype={'cod_prod_acabado': str}\n",
    ")\n",
    "bd_estruturas['cod_prod_acabado'] = bd_estruturas['cod_prod_acabado'].astype(str).str.strip().str.upper()\n",
    "bd_estruturas['cod_insumo'] = bd_estruturas['cod_insumo'].astype(str).str.strip().str.upper()\n",
    "bd_estruturas = bd_estruturas[bd_estruturas['cod_prod_acabado'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "bd_estruturas = bd_estruturas[colunas_bd_estruturas]\n",
    "# Elminar coluna mes_ref e remover duplicatas\n",
    "bd_estruturas = bd_estruturas.drop(columns=['mes_ref'])\n",
    "bd_estruturas = bd_estruturas.drop_duplicates(subset=['empresa', 'cod_prod_acabado', 'cod_insumo']).reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Mapeamento de pastas e importação de tabelas concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f84cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Geração de tabelas de calendário concluído!\n"
     ]
    }
   ],
   "source": [
    "# Gerando a tabela Calendário\n",
    "\n",
    "# Definir intervalo de datas: do primeiro mês ao último mês completo\n",
    "data_inicial = bd_prod['mes_ref'].min().replace(day=1)\n",
    "data_final = bd_prod['mes_ref'].max() + MonthEnd(1)\n",
    "\n",
    "# Gerar datas diariamente entre data_inicial e data_final\n",
    "datas = pd.date_range(start=data_inicial, end=data_final, freq='D')\n",
    "\n",
    "# Mapeamento dos dias da semana\n",
    "dias_semana = {0: 'SEG', 1: 'TER', 2: 'QUA', 3: 'QUI', 4: 'SEX', 5: 'SÁB', 6: 'DOM'}\n",
    "\n",
    "# Construção do DataFrame\n",
    "df_calendario = pd.DataFrame({\n",
    "    'data_calend': datas,\n",
    "    'mes_calend': datas.to_series().apply(lambda d: d.replace(day=1)),\n",
    "    'dia_calend': datas.to_series().dt.weekday.map(dias_semana)\n",
    "})\n",
    "\n",
    "# Importar aba Dia_Semana da planilha dados_calendario, com dados por dia da semana x Unidade\n",
    "df_dia_semana = pd.read_excel(pasta_input / 'dados_calendario.xlsx', engine='calamine', sheet_name='Dia_Semana')\n",
    "\n",
    "# Normalização (unpivot)\n",
    "bd_dia_semana = df_dia_semana.melt(\n",
    "    id_vars=['UNIDADE'],\n",
    "    var_name='DIA_DE_SEMANA',\n",
    "    value_name='OCUPACAO'\n",
    ")\n",
    "\n",
    "# Remove linhas onde OCUPACAO está vazia ou inválida (ex: '-')\n",
    "bd_dia_semana = bd_dia_semana[bd_dia_semana['OCUPACAO'].notna()]\n",
    "bd_dia_semana = bd_dia_semana[bd_dia_semana['OCUPACAO'] != '-']\n",
    "bd_dia_semana['OCUPACAO'] = bd_dia_semana['OCUPACAO'].astype(float)\n",
    "\n",
    "# Fazendo o merge entre o calendário e os dados de ocupação por unidade/dia da semana\n",
    "df_calendario_ocup = df_calendario.merge(\n",
    "    bd_dia_semana,\n",
    "    left_on='dia_calend',\n",
    "    right_on='DIA_DE_SEMANA',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Selecionar apenas as colunas desejadas\n",
    "df_calendario_ocup = df_calendario_ocup[\n",
    "    ['data_calend', 'mes_calend', 'dia_calend', 'UNIDADE', 'OCUPACAO']\n",
    "]\n",
    "\n",
    "# Renomear colunas para maiúsculas\n",
    "df_calendario_ocup.columns = [col.upper() for col in df_calendario_ocup.columns]\n",
    "\n",
    "# Importar aba Feriados da planilha dados_calendario, com dados por dia da semana x Unidade\n",
    "df_feriados = pd.read_excel(pasta_input / 'dados_calendario.xlsx', engine='calamine', sheet_name='Feriados')\n",
    "\n",
    "# Garantir que as datas estejam no mesmo formato\n",
    "df_feriados['FERIADO'] = pd.to_datetime(df_feriados['FERIADO'], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Realizar o merge\n",
    "df_calendario_ocup = df_calendario_ocup.merge(\n",
    "    df_feriados[['FERIADO', 'UNIDADE', 'TIPO', 'NORMAL', 'REVEZAMENTO']],\n",
    "    left_on=['DATA_CALEND', 'UNIDADE'],\n",
    "    right_on=['FERIADO', 'UNIDADE'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "data_agora = datetime.now()\n",
    "# Inserindo coluna data e hora agora\n",
    "df_calendario_ocup['DATA_AGORA'] = data_agora\n",
    "\n",
    "# NOVO_NORMAL: Se TIPO for NaN, retorna OCUPACAO; senão, retorna TIPO NORMAL\n",
    "df_calendario_ocup['NOVO_NORMAL'] = df_calendario_ocup.apply(lambda row: row['OCUPACAO'] if pd.isna(row['TIPO']) else row['NORMAL'],axis=1)\n",
    "\n",
    "# NOVO_REVEZAMENTO: Se TIPO for NaN, retorna 1; senão, retorna TIPO REVEZAMENTO\n",
    "df_calendario_ocup['NOVO_REVEZAMENTO'] = df_calendario_ocup.apply(lambda row: 1 if pd.isna(row['TIPO']) else row['REVEZAMENTO'],axis=1)\n",
    "\n",
    "# Coluna DIAS_NOR_CALEND: se DATA_CALEND <= DATA_AGORA retorna 0, senão NOVO_NORMAL\n",
    "df_calendario_ocup['DIAS_NOR_CALEND_'] = df_calendario_ocup.apply(lambda row: 0 if row['DATA_CALEND'] <= row['DATA_AGORA'] else row['NOVO_NORMAL'],axis=1)\n",
    "\n",
    "# Coluna DIAS_REV_CALEND: se DATA_CALEND <= DATA_AGORA retorna 0, senão NOVO_REVEZAMENTO\n",
    "df_calendario_ocup['DIAS_REV_CALEND_'] = df_calendario_ocup.apply(lambda row: 0 if row['DATA_CALEND'] <= row['DATA_AGORA'] else row['NOVO_REVEZAMENTO'],axis=1)\n",
    "\n",
    "# Coluna PARCIAL_HOJE: se DATA_CALEND = DATA_AGORA, RETORNA 1 - (Hora Agora / 24)\n",
    "hora_agora = data_agora.hour + data_agora.minute / 60 + data_agora.second / 3600\n",
    "data_hoje = data_agora.date()\n",
    "\n",
    "df_calendario_ocup['PARCIAL_HOJE'] = df_calendario_ocup['DATA_CALEND'].dt.date.apply(lambda d: 1 - (hora_agora / 24) if d == data_hoje else 0)\n",
    "\n",
    "df_calendario_ocup['DIAS_NOR_CALEND'] = df_calendario_ocup['DIAS_NOR_CALEND_'] + df_calendario_ocup['PARCIAL_HOJE']\n",
    "df_calendario_ocup['DIAS_REV_CALEND'] = df_calendario_ocup['DIAS_REV_CALEND_'] + df_calendario_ocup['PARCIAL_HOJE']\n",
    "\n",
    "bd_calend = df_calendario_ocup.groupby(['MES_CALEND', 'UNIDADE'], as_index=False)[['DIAS_NOR_CALEND', 'DIAS_REV_CALEND']].sum()\n",
    "bd_calend = bd_calend.sort_values(by=['MES_CALEND', 'UNIDADE'])\n",
    "bd_calend = bd_calend.rename(columns={\n",
    "    'DIAS_NOR_CALEND': 'TOT_DIAS_NOR_CALEND',\n",
    "    'DIAS_REV_CALEND': 'TOT_DIAS_REV_CALEND'\n",
    "})\n",
    "\n",
    "bd_calend['TOT_HORAS_NOR_CALEND'] = bd_calend['TOT_DIAS_NOR_CALEND'] * 24\n",
    "bd_calend['TOT_HORAS_REV_CALEND'] = bd_calend['TOT_DIAS_REV_CALEND'] * 24\n",
    "\n",
    "print(\"✅ Geração de tabelas de calendário concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afd7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicio Step = GERAR DADOS LTP\n",
    "bd_prod['NEC_BASE_PROD_PCS'] = (\n",
    "    bd_prod['estoq_inicial_pcs'] \n",
    "    - bd_prod['carteira_mes_atual'] \n",
    "    - bd_prod['carteira_arraste_mes_anterior'] \n",
    "    - bd_prod['previsao_pcs'] \n",
    "    - bd_prod['estoq_seg_pcs']\n",
    ")\n",
    "\n",
    "bd_prod['PRIORIDADE'] = 1\n",
    "\n",
    "bd_mat_param_prioridade_1 = bd_mat_param[bd_mat_param['prioridade'] == 1]\n",
    "\n",
    "# Join completo entre bd_prod e bd_mat_param\n",
    "bd_prod_nec = pd.merge(\n",
    "    bd_mat_param_prioridade_1,\n",
    "    bd_prod,\n",
    "    left_on=['produto', 'unidade_fat', 'prioridade'],\n",
    "    right_on=['cod_produto', 'empresa', 'PRIORIDADE'],\n",
    "    how='outer'  # FULL OUTER JOIN\n",
    ")\n",
    "\n",
    "# Importar aba matriz_regioes da planilha matriz regioes\n",
    "bd_matriz_regioes = pd.read_excel(pasta_input / 'matriz_regioes.xlsx', sheet_name='matriz_regioes', engine='calamine')\n",
    "\n",
    "# Merge para adicionar a Região  de Faturamento na base de produtos\n",
    "bd_prod_nec = bd_prod_nec.merge(\n",
    "    bd_matriz_regioes,\n",
    "    how='left',\n",
    "    left_on='empresa',\n",
    "    right_on='Unidade'\n",
    ")\n",
    "\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Unidade': 'Unidade_Fat'})\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Região': 'Reg_Unid_Fat'})\n",
    "\n",
    "# Merge para adicionar a Região Unidade Produtiva na base de produtos\n",
    "bd_prod_nec = bd_prod_nec.merge(\n",
    "    bd_matriz_regioes,\n",
    "    how='left',\n",
    "    left_on='unidade_prod',\n",
    "    right_on='Unidade'\n",
    ")\n",
    "\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Unidade': 'Unidade_Prod'})\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Região': 'Reg_Unid_Prod'})\n",
    "\n",
    "# Criando a coluna MESMA REGIAO, com os critérios aplicados\n",
    "bd_prod_nec['MESMA_REG'] = bd_prod_nec.apply(\n",
    "    lambda row: \"NAO\" if pd.isna(row['Reg_Unid_Prod']) else (\"SIM\" if row['Reg_Unid_Fat'] == row['Reg_Unid_Prod'] else \"NAO\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Inserindo a coluna Mes_Ref_Ant na bd_prod, e criando uma bd_prod_mes_anterior\n",
    "bd_prod_mes_anterior = bd_prod[['mes_ref', 'empresa', 'cod_produto', 'descricao', 'saldo_previsao_pcs']].copy()\n",
    "bd_prod_mes_anterior = bd_prod_mes_anterior.rename(columns=\n",
    "    {'mes_ref': 'MES_REF', \n",
    "     'empresa': 'EMPRESA', \n",
    "     'cod_produto': 'COD_PROD', \n",
    "     'descricao': 'DESC_PROD',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PROX_MES_PCS'\n",
    "     }\n",
    ")\n",
    "\n",
    "# Criando coluna MES_REF_ANT para definir mes do saldo da previsão\n",
    "bd_prod_mes_anterior['MES_REF_ANT'] = (bd_prod_mes_anterior['MES_REF'] - pd.DateOffset(months=1)).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Adicionar a coluna SALDO_PREV_PROX_MES_PCS na bd_prod_nec\n",
    "# Criando um dicionário com as chaves de busca e a coluna que você quer trazer\n",
    "map_dict = bd_prod_mes_anterior.set_index(['MES_REF_ANT', 'EMPRESA', 'COD_PROD'])['SALDO_PREV_PROX_MES_PCS'].to_dict()\n",
    "\n",
    "# Usando map para trazer a coluna 'SALDO_PREV_PROX_MES_PCS' para bd_prod_nec\n",
    "bd_prod_nec['SALDO_PREV_PROX_MES_PCS'] = bd_prod_nec[['mes_ref', 'empresa', 'cod_produto']].apply(\n",
    "    lambda row: map_dict.get((row['mes_ref'], row['empresa'], row['cod_produto']), None), axis=1\n",
    ")\n",
    "\n",
    "# Eliminando linhas filtrando mes_ref que não seja null\n",
    "bd_prod_nec = bd_prod_nec[bd_prod_nec['mes_ref'].notna()]\n",
    "\n",
    "# Criando a coluna PCS_NEC_PROD_MESMA_REG_SIM_NAO\n",
    "# Preencher todos os NaN/None das colunas usadas no cálculo com zero\n",
    "colunas_nec = [\n",
    "    'carteira_arraste_mes_anterior', 'carteira_mes_atual', 'saldo_previsao_pcs',\n",
    "    'SALDO_PREV_PROX_MES_PCS', 'estoq_seg_pcs', 'estoq_inicial_pcs', 'estoq_transf_pcs'\n",
    "]\n",
    "for col in colunas_nec:\n",
    "    bd_prod_nec[col] = pd.to_numeric(bd_prod_nec[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Agora pode aplicar o cálculo normalmente\n",
    "bd_prod_nec['PCS_NEC_PROD_MESMA_REG_SIM_NAO'] = bd_prod_nec.apply(\n",
    "    lambda row: (\n",
    "        row['carteira_arraste_mes_anterior'] + row['carteira_mes_atual'] + row['saldo_previsao_pcs'] + row['SALDO_PREV_PROX_MES_PCS'] + row['estoq_seg_pcs']\n",
    "        - (row['estoq_inicial_pcs'] + row['estoq_transf_pcs'])\n",
    "    ) if row['tipo_produto'] == 'MR'\n",
    "    else (\n",
    "        row['carteira_arraste_mes_anterior'] + row['carteira_mes_atual'] + row['saldo_previsao_pcs'] + row['estoq_seg_pcs']\n",
    "        - (row['estoq_inicial_pcs'] + row['estoq_transf_pcs'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Aplicar substituir null por Zero na coluna SALDO_PREV_PROX_MES_PCS\n",
    "bd_prod_nec['SALDO_PREV_PROX_MES_PCS'] = bd_prod_nec['SALDO_PREV_PROX_MES_PCS'].fillna(0)\n",
    "\n",
    "# Padronizando nomes de colunas para concluir bd_prod_nec\n",
    "bd_prod_nec = bd_prod_nec.rename(columns=\n",
    "    {'mes_ref': 'MES_REF', \n",
    "     'empresa': 'EMPRESA', \n",
    "     'cod_produto': 'COD_PROD', \n",
    "     'descricao': 'DESC_PROD',\n",
    "     'linha_prod': 'LINHA_PROD',\n",
    "     'familia_prod': 'FAMILIA_PROD',\n",
    "     'tipo_produto': 'TIPO_PROD',\n",
    "     'curva_abc': 'CURVA_ABC',\n",
    "     'curva_123': 'CURVA_123',\n",
    "     'estoq_seg_pcs': 'EST_SEG_PCS',\n",
    "     'estoq_inicial_pcs': 'EST_INI_PCS',\n",
    "     'carteira_arraste_mes_anterior': 'CART_ARR_MES_ANT',\n",
    "     'carteira_mes_atual': 'CART_MES_ATUAL',\n",
    "     'previsao_pcs': 'PREV_PCS',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PCS',\n",
    "     'peso_produto_kg': 'PESO_PROD_KG',\n",
    "     'estoq_transf_pcs': 'EST_TRANS_PCS',\n",
    "     'unidade_prod': 'UNID_PROD',\n",
    "     'MESMA_REG': 'MESMA_REG',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PCS',\n",
    "     'lote_econ': 'LOTE_ECON',\n",
    "     'qtd_emb': 'QTD_EMB',\n",
    "     'lote_min': 'LOTE_MIN',\n",
    "     }\n",
    ")\n",
    "\n",
    "# Definindo colunas e encerrando processo de formação da bd_prod_nec\n",
    "bd_prod_nec = bd_prod_nec[\n",
    "    ['MES_REF', 'EMPRESA', 'UNID_PROD', 'MESMA_REG', 'COD_PROD', 'DESC_PROD', \n",
    "     'LINHA_PROD', 'FAMILIA_PROD', 'TIPO_PROD', 'CURVA_ABC','CURVA_123', 'LOTE_ECON','QTD_EMB', 'LOTE_MIN', \n",
    "     'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'PREV_PCS', \n",
    "     'SALDO_PREV_PCS', 'PESO_PROD_KG', 'EST_TRANS_PCS','PCS_NEC_PROD_MESMA_REG_SIM_NAO',\n",
    "     'SALDO_PREV_PROX_MES_PCS']\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c06cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Geração de tabelas de dados LTP concluída!\n"
     ]
    }
   ],
   "source": [
    "### Transformação 04_GerarBaseLTP para gerar a base LTP\n",
    "\n",
    "# Carregar base_dados_roteiros\n",
    "bd_rot = pd.read_excel(\n",
    "    pasta_input / 'base_dados_roteiros.xlsx',\n",
    "    sheet_name='base_dados_roteiros',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "\n",
    "bd_rot['cod_produto'] = bd_rot['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_rot = bd_rot[bd_rot['cod_produto'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "\n",
    "if 'mes_ref' in bd_rot.columns:\n",
    "    bd_rot['mes_ref'] = pd.to_datetime(bd_rot['mes_ref'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "colunas_manter = ['mes_ref', 'empresa', 'cod_produto', 'descricao', 'linha_prod', 'familia_prod', 'tipo_produto', 'cod_ferramenta', 'n_cavidade', 'ciclo_molde', 'pcs_hr', 'mo', 'alocacao_recurso', 'grupo_setor', 'prioridade']\n",
    "\n",
    "bd_rot = bd_rot[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Adicionando nova coluna na tabela de roteiros\n",
    "bd_rot['COD_FERR'] = (bd_rot['cod_ferramenta'] + \"|\" + bd_rot['empresa'].str[:3]).str.upper()\n",
    "\n",
    "# Faz o merge trazendo somente as 4 colunas desejadas, associando pelos campos de chave\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa'], \n",
    "    right_on=['MES_CALEND', 'UNIDADE']\n",
    ")\n",
    "\n",
    "# Removendo as colunas que não quero na bd_rot\n",
    "colunas_excluir = ['MES_CALEND', 'UNIDADE']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Carregando o Calendario de Recursos\n",
    "bd_calend_rec = pd.read_excel(pasta_input / 'calend_recursos.xlsx', sheet_name='calend_recursos')\n",
    "\n",
    "# Inserindo coluna PER_TURNO no calendário de recursos\n",
    "bd_calend_rec['PER_TURNO'] = bd_calend_rec['turno_maq'] / 3\n",
    "\n",
    "# Inserindo colunas do calendário de recursos na bd_rot\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend_rec, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa', 'alocacao_recurso'], \n",
    "    right_on=['mes_ref', 'unidade', 'maq']\n",
    ")\n",
    "\n",
    "colunas_excluir = ['unidade', 'maq', 'setores', 'turno_maq']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "bd_rot.rename(columns={\n",
    "    'revezamento': 'REC_revezamento',\n",
    "    'horas_extras': 'REC_horas_extras',\n",
    "    'disp_mes': 'REC_disp_mes',\n",
    "    'horas_indisp': 'REC_horas_indisp',\n",
    "    'PER_TURNO': 'REC_PER_TURNO'\n",
    "}, inplace=True)\n",
    "\n",
    "# Carregando Calendário de Ferramentas\n",
    "bd_calend_ferr = pd.read_excel(pasta_input / 'calend_ferramentas.xlsx', sheet_name='calend_ferramentas')\n",
    "bd_calend_ferr['NEW_MOLDES'] = bd_calend_ferr['moldes'].str.upper() + \"|\" + bd_calend_ferr['unidade'].str.upper()\n",
    "\n",
    "# Merge entre bd_rot e bd_calend_ferr\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend_ferr, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa', 'cod_ferramenta'], \n",
    "    right_on=['mes_ref', 'unidade', 'moldes']\n",
    ")\n",
    "\n",
    "colunas_excluir = ['moldes','unidade', 'setores']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "bd_rot.rename(columns={\n",
    "    'horas_indisp': 'FER_horas_indisp',\n",
    "    'disp_mes': 'FER_disp_mes',\n",
    "    'NEW_MOLDES': 'moldes'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preencher apenas os valores nulos de FER_horas_indisp com zero\n",
    "bd_rot['FER_horas_indisp'] = bd_rot['FER_horas_indisp'].fillna(0)\n",
    "\n",
    "# Calculando coluna HOR_REC_C1\n",
    "# Se REC_revezamento for \"SIM\", calcula com TOT_HORAS_REV_CALEND, senão com TOT_HORAS_NOR_CALEND\n",
    "# Calculando coluna HOR_REC_C1, se algum valor for nulo, retorna 0\n",
    "bd_rot['HOR_REC_C1'] = np.where(\n",
    "    bd_rot[['TOT_HORAS_REV_CALEND', 'REC_horas_extras', 'REC_horas_indisp', 'REC_PER_TURNO', 'REC_disp_mes', 'TOT_HORAS_NOR_CALEND']].isnull().any(axis=1),\n",
    "    0,\n",
    "    np.where(\n",
    "        bd_rot['REC_revezamento'] == \"SIM\",\n",
    "        ((bd_rot['TOT_HORAS_REV_CALEND'] + bd_rot['REC_horas_extras'] - bd_rot['REC_horas_indisp']) * bd_rot['REC_PER_TURNO']) * bd_rot['REC_disp_mes'],\n",
    "        ((bd_rot['TOT_HORAS_NOR_CALEND'] + bd_rot['REC_horas_extras'] - bd_rot['REC_horas_indisp']) * bd_rot['REC_PER_TURNO']) * bd_rot['REC_disp_mes']\n",
    "    )\n",
    ").round(2)\n",
    "\n",
    "# Calculando coluna HOR_FERR_C1\n",
    "# Se REC_revezamento for \"SIM\", calcula com TOT_HORAS_REV_CALEND, senão com TOT_HORAS_NOR_CALEND\n",
    "# Calculando coluna HOR_FER_C1, se algum valor for nulo, retorna 0, senão calcula conforme regra, com duas casas decimais\n",
    "bd_rot['HOR_FER_C1'] = np.where(\n",
    "    bd_rot[['TOT_HORAS_REV_CALEND', 'REC_horas_extras', 'FER_horas_indisp', 'TOT_HORAS_NOR_CALEND', 'FER_disp_mes']].isnull().any(axis=1),\n",
    "    0,\n",
    "    np.where(\n",
    "        bd_rot['REC_revezamento'] == \"SIM\",\n",
    "        (bd_rot['TOT_HORAS_REV_CALEND'] + (bd_rot['REC_horas_extras'] - bd_rot['FER_horas_indisp'])) * bd_rot['FER_disp_mes'],\n",
    "        (bd_rot['TOT_HORAS_NOR_CALEND'] + (bd_rot['REC_horas_extras'] - bd_rot['FER_horas_indisp'])) * bd_rot['FER_disp_mes']\n",
    "    )\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_REC conforme a regra\n",
    "# Se HOR_REC_C1 for null ou menor que 0, atribui 0; senão, atribui o valor de HOR_REC_C1\n",
    "bd_rot['HOR_REC'] = np.where(\n",
    "    bd_rot['HOR_REC_C1'].isnull() | (bd_rot['HOR_REC_C1'] < 0),\n",
    "    0,\n",
    "    bd_rot['HOR_REC_C1']\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_FER conforme a regra\n",
    "# Se HOR_FER_C1 for null ou menor que 0, atribui 0; senão, atribui o valor de HOR_FER_C1\n",
    "bd_rot['HOR_FER'] = np.where(\n",
    "    bd_rot['HOR_FER_C1'].isnull() | (bd_rot['HOR_FER_C1'] < 0),\n",
    "    0,\n",
    "    bd_rot['HOR_FER_C1']\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_CAP conforme a regra\n",
    "# Se HOR_REC for menor que HOR_FER, atribui HOR_REC; senão, atribui HOR_FER\n",
    "bd_rot['HOR_CAP'] = np.where(\n",
    "    bd_rot['HOR_REC'] < bd_rot['HOR_FER'],\n",
    "    bd_rot['HOR_REC'],\n",
    "    bd_rot['HOR_FER']\n",
    ").round(2)\n",
    "\n",
    "# Eliminando colunas para tornar mais clean a bd_rot\n",
    "colunas_excluir = [\n",
    "    'REC_revezamento', 'REC_horas_extras', 'REC_disp_mes', 'REC_horas_indisp',\n",
    "    'TOT_HORAS_REV_CALEND', 'TOT_HORAS_NOR_CALEND', 'REC_PER_TURNO',\n",
    "    'FER_horas_indisp', 'HOR_REC_C1', 'HOR_FER_C1', 'disp_mes'\n",
    "]\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando a coluna prioridade do bd_rot para PRIORIDADE_ROTEIRO, para evitar confusão \n",
    "# com a prioridade da bd_mat_param\n",
    "bd_rot.rename(columns={'prioridade': 'PRIORIDADE_ROTEIRO'}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_mat_param para trazer os campos unidade_fat e prioridade\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_mat_param, \n",
    "    how='left', \n",
    "    left_on=['empresa', 'cod_produto'], \n",
    "    right_on=['unidade_prod', 'produto']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = ['produto', 'unidade_prod', 'tipo_abast']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando colunas da matriz parametros que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'unidade_fat': 'MAT_PAR_unidade_fat',\n",
    "    'prioridade': 'MAT_PAR_prioridade'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_prod_necs para trazer os campos de volumese e dados\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_prod_nec, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'MAT_PAR_unidade_fat', 'cod_produto'], \n",
    "    right_on=['MES_REF', 'EMPRESA', 'COD_PROD']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = [\n",
    "    'TOT_DIAS_NOR_CALEND','TOT_DIAS_REV_CALEND', 'moldes', 'MES_REF', 'EMPRESA',\n",
    "    'UNID_PROD', 'COD_PROD', 'DESC_PROD', 'LINHA_PROD',\n",
    "    'FAMILIA_PROD', 'TIPO_PROD', 'CURVA_ABC', 'CURVA_123'\n",
    "]\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando da bd_roteiro que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'mes_ref': 'MES_REF',\n",
    "    'empresa': 'EMPRESA',\n",
    "    'cod_produto': 'COD_PROD',\n",
    "    'descricao': 'DESC_PROD',\n",
    "    'linha_prod': 'LINHA_PROD',\n",
    "    'familia_prod': 'FAMILIA_PROD',\n",
    "    'tipo_produto': 'TIPO_PROD',\n",
    "    'COD_FERR': 'COD_FER_UNID',\n",
    "    'cod_ferramenta': 'COD_FERR',\n",
    "    'n_cavidade': 'N_CAVIDADE',\n",
    "    'ciclo_molde': 'CICLO_MOLDE',\n",
    "    'pcs_hr': 'PCS_HORA',\n",
    "    'mo': 'MO',\n",
    "    'alocacao_recurso': 'ALOC_REC',\n",
    "    'grupo_setor': 'GRUPO_SETOR',\n",
    "    'peso_produto_kg': 'PESO_PROD',\n",
    "    'PRIORIDADE_ROTEIRO': 'PRIOR_ROT',\n",
    "    'MAT_PAR_unidade_fat': 'UNID_FAT_MATPAR',\n",
    "    'MAT_PAR_prioridade': 'PRIOR_MATPAR'\n",
    "}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_regioes para trazer os campo região fat\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_regioes, \n",
    "    how='left', \n",
    "    left_on=['UNID_FAT_MATPAR'], \n",
    "    right_on=['Unidade']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "bd_rot = drop_colunas(bd_rot, ['Unidade'])\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'Região': 'REG_UNID_FAT'}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_regioes para trazer os campo região prod\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_regioes, \n",
    "    how='left', \n",
    "    left_on=['EMPRESA'], \n",
    "    right_on=['Unidade']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "bd_rot = drop_colunas(bd_rot, ['Unidade'])\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'Região': 'REG_UNID_PROD'}, inplace=True)\n",
    "\n",
    "# Criando o campo MESMA_REG conforme a regra solicitada\n",
    "bd_rot['MESMA_REG'] = np.where(\n",
    "    bd_rot['REG_UNID_PROD'].isna(),\n",
    "    \"NAO\",\n",
    "    np.where(\n",
    "        bd_rot['REG_UNID_FAT'] == bd_rot['REG_UNID_PROD'],\n",
    "        \"SIM\",\n",
    "        \"NAO\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Substituir null (NaN) por 0 nos campos solicitados\n",
    "campos_zerar = [\n",
    "    'EST_SEG_PCS', 'EST_INI_PCS',\n",
    "    'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'PREV_PCS', 'SALDO_PREV_PCS',\n",
    "    'PESO_PROD_KG', 'EST_TRANS_PCS', 'SALDO_PREV_PROX_MES_PCS', 'PCS_NEC_PROD_MESMA_REG_SIM_NAO'\n",
    "]\n",
    "bd_rot[campos_zerar] = bd_rot[campos_zerar].fillna(0)\n",
    "\n",
    "# Criando a coluna MES_REF_ANT reduzindo 1 mês de MES_REF\n",
    "bd_rot['MES_REF_ANT'] = (bd_rot['MES_REF'] - pd.DateOffset(months=1)).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Renomeando da bd_roteiro que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'EMPRESA': 'UNID_PROD',\n",
    "    'UNID_FAT_MATPAR': 'UNID_FAT',\n",
    "}, inplace=True)\n",
    "\n",
    "# Importar aba matriz_regioes da planilha matriz limitantes\n",
    "bd_matriz_limitantes = pd.read_excel(pasta_input / 'matriz_limitantes.xlsx', sheet_name='matriz_limitantes', dtype={'produto': str})\n",
    "\n",
    "if 'mes_ref' in bd_matriz_limitantes.columns:\n",
    "    bd_matriz_limitantes['mes_ref'] = pd.to_datetime(bd_matriz_limitantes['mes_ref'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_limitantes para trazer os campo limitante_pcs\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_limitantes, \n",
    "    how='left',\n",
    "    left_on=['MES_REF','UNID_FAT','COD_PROD'],\n",
    "    right_on=['mes_ref','unidade','produto']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = ['mes_ref','unidade','produto']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'limitante_pcs': 'LIMIT_PCS'}, inplace=True)\n",
    "\n",
    "# Aplicando a regra no campo LIMIT_PCS conforme lógica Java\n",
    "bd_rot['AVALIAR'] = bd_rot['PRIOR_MATPAR'].astype(str) + \"|\" + bd_rot['PRIOR_ROT'].astype(str)\n",
    "bd_rot['LIMIT_PCS'] = np.where(\n",
    "    (bd_rot['AVALIAR'] != \"1|1\") | (bd_rot['LIMIT_PCS'].isnull()),\n",
    "    0,\n",
    "    bd_rot['LIMIT_PCS']\n",
    ")\n",
    "\n",
    "# Formatando a coluna PRIOR_MATPAR para inteiro\n",
    "bd_rot['PRIOR_MATPAR'] = bd_rot['PRIOR_MATPAR'].fillna(0).astype(int)\n",
    "\n",
    "# Eliminar a coluna AVALIAR\n",
    "colunas_excluir = ['AVALIAR', 'MES_REF_ANT']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Criar ID_COMP\n",
    "bd_rot['ID_COMP'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_PROD'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_RECURSO\n",
    "bd_rot['ID_RECURSO'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['ALOC_REC'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_FERRAMENTA\n",
    "bd_rot['ID_FERRAMENTA'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_FER_UNID'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_PROD_UNID_FAT\n",
    "bd_rot['ID_PROD_UNID_FAT'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_FAT'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_PROD'].astype(str).str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_PROD_UNID_FAT_ANT já calculando o mês anterior no mesmo comando\n",
    "bd_rot['ID_PROD_UNID_FAT_ANT'] = (\n",
    "    (bd_rot['MES_REF'] - pd.DateOffset(months=1)).dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_FAT'].str.upper()  + \"|\" +\n",
    "    bd_rot['COD_PROD'].str.upper()\n",
    ")\n",
    " \n",
    "# Criar a coluna IND\n",
    "# Classificação antiga\n",
    "# bd_rot = bd_rot.sort_values(by=['MES_REF', 'UNID_FAT', 'COD_PROD', 'PRIOR_MATPAR', 'PRIOR_ROT', 'TIPO_PROD']).reset_index(drop=True)\n",
    "\n",
    "# Classificação nova\n",
    "bd_rot = bd_rot.sort_values(by=['MES_REF', 'TIPO_PROD', 'UNID_FAT', 'COD_PROD', 'PRIOR_MATPAR', 'PRIOR_ROT']).reset_index(drop=True)\n",
    "bd_rot['IND'] = range(1, len(bd_rot) + 1)\n",
    "\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_REC ||||||||||||||||||||||||||||||||\n",
    "# Remover linhas onde ID_RECURSO é NaN\n",
    "bd_rot = bd_rot[bd_rot['ID_RECURSO'].notna()].copy()\n",
    "\n",
    "# Ordena primeiro por ID_RECURSO e depois por IND\n",
    "# bd_rot = bd_rot.sort_values(by=['ID_RECURSO', 'IND']).reset_index(drop=True)\n",
    "\n",
    "# Cria o índice incremental por ID_RECURSO, começando em 1\n",
    "# bd_rot['ID_NUM_REC'] = (bd_rot.groupby('ID_RECURSO').cumcount() + 1).astype(int)\n",
    "\n",
    "# Classificar crescente pela coluna IND\n",
    "bd_rot = bd_rot.sort_values(by=['IND']).reset_index(drop=True)\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_REC ||||||||||||||||||||||||||||||||\n",
    "\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_FER ||||||||||||||||||||||||||||||||\n",
    "# Remover linhas onde ID_FERRAMENTA é NaN\n",
    "bd_rot = bd_rot[bd_rot['ID_FERRAMENTA'].notna()].copy()\n",
    "\n",
    "# # Ordena primeiro por ID_NUM_FER e depois por IND\n",
    "# bd_rot = bd_rot.sort_values(by=['ID_FERRAMENTA', 'IND']).reset_index(drop=True)\n",
    "\n",
    "# # Cria o índice incremental por ID_NUM_FER, começando em 1\n",
    "# bd_rot['ID_NUM_FER'] = (bd_rot.groupby('ID_FERRAMENTA').cumcount() + 1).astype(int)\n",
    "\n",
    "# Classificar crescente pela coluna IND\n",
    "bd_rot = bd_rot.sort_values(by=['IND']).reset_index(drop=True)\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_FER ||||||||||||||||||||||||||||||||\n",
    "\n",
    "# Criar coluna ID_CONC_REC = ID_RECURSO + \"|\" + ID_NUM_REC e ID_CONC_FER = ID_FERRAMENTA + \"|\" + ID_NUM_FER\n",
    "# bd_rot['ID_CONC_REC'] = bd_rot['ID_RECURSO'] + \"|\" + bd_rot['ID_NUM_REC'].astype(str)\n",
    "# bd_rot['ID_CONC_FER'] = bd_rot['ID_FERRAMENTA'] + \"|\" + bd_rot['ID_NUM_FER'].astype(str)\n",
    "\n",
    "# Organizando Layout Final da para dados para bd_base_LTP\n",
    "# Ordenando colunas\n",
    "nova_ordem_colunas = [\n",
    "    'MES_REF', 'UNID_FAT', 'UNID_PROD', 'MESMA_REG', 'PRIOR_MATPAR', 'PRIOR_ROT', 'COD_PROD', 'DESC_PROD', 'LINHA_PROD',\n",
    "    'FAMILIA_PROD', 'TIPO_PROD', 'COD_FER_UNID', 'N_CAVIDADE', 'CICLO_MOLDE', 'PCS_HORA', 'MO', 'ALOC_REC', \n",
    "    'GRUPO_SETOR', 'PESO_PROD_KG', 'LOTE_ECON', 'QTD_EMB','LOTE_MIN', 'IND', 'ID_COMP', 'ID_RECURSO', 'ID_FERRAMENTA', 'ID_PROD_UNID_FAT', 'ID_PROD_UNID_FAT_ANT', 'HOR_REC', 'HOR_FER', 'HOR_CAP', 'PREV_PCS', 'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'SALDO_PREV_PCS', 'EST_TRANS_PCS', 'SALDO_PREV_PROX_MES_PCS', 'LIMIT_PCS'\n",
    "    ]\n",
    "\n",
    "# # Reordenando as colunas\n",
    "bd_prod_rot = bd_rot[nova_ordem_colunas].reset_index(drop=True)\n",
    "\n",
    "# Criar tabela com ID_PROD_UNID_FAT e MESMA_REG, filtrando PRIOR_MATPAR = 1 e PRIOR_ROT = 1\n",
    "bd_prod_rot_PRIOR_11 = bd_prod_rot[\n",
    "    (bd_prod_rot['PRIOR_MATPAR'] == 1) & (bd_prod_rot['PRIOR_ROT'] == 1)\n",
    "][['ID_PROD_UNID_FAT', 'MESMA_REG']].drop_duplicates().reset_index(drop=True)\n",
    "bd_prod_rot_PRIOR_11.rename(columns={'MESMA_REG': 'MESMA_REG_PRIOR_11'}, inplace=True)\n",
    "\n",
    "# Fazer join entre bd_prod_rot e bd_prod_rot_PRIOR_11 para trazer MESMA_REG_PRIOR_11\n",
    "bd_prod_rot = bd_prod_rot.merge(\n",
    "    bd_prod_rot_PRIOR_11,\n",
    "    how='left',\n",
    "    on='ID_PROD_UNID_FAT'\n",
    ")\n",
    "\n",
    "# Substituir coluna MESMA_REG por MESMA_REG_PRIOR_11\n",
    "bd_prod_rot['MESMA_REG'] = bd_prod_rot['MESMA_REG_PRIOR_11'].fillna(bd_prod_rot['MESMA_REG'])\n",
    "# Eliminar coluna MESMA_REG_PRIOR_11\n",
    "bd_prod_rot = drop_colunas(bd_prod_rot, ['MESMA_REG_PRIOR_11'])\n",
    "\n",
    "# Criar bd_estrutura_filtrada, com base na bd_estruturas eliminando cod_insumo que não constam na coluna COD_PROD da bd_prod_rot\n",
    "# codigos_validos = set(bd_prod_rot['COD_PROD'].unique())\n",
    "# bd_estrutura_filtrada = bd_estruturas[bd_estruturas['cod_insumo'].isin(codigos_validos)].copy().reset_index(drop=True)\n",
    "\n",
    "# Criar conjunto de tuplas válidas (COD_PROD, UNID_PROD)\n",
    "codigos_validos = set(zip(bd_prod_rot['COD_PROD'], bd_prod_rot['UNID_PROD']))\n",
    "\n",
    "# Filtrar estrutura com base em cod_insumo + empresa\n",
    "bd_estrutura_filtrada = (\n",
    "    bd_estruturas[\n",
    "        bd_estruturas[['cod_insumo', 'empresa']].apply(tuple, axis=1).isin(codigos_validos)\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Mesclar bd_base_LTP e bd_estrutura_filtrada por UNID_PROD, COD_PROD vs empresa e cod_prod_acabado\n",
    "bd_prod_rot_estr = pd.merge(bd_prod_rot, bd_estrutura_filtrada, how='left', left_on=['UNID_PROD', 'COD_PROD'], right_on=['empresa', 'cod_prod_acabado']).reset_index(drop=True)\n",
    "\n",
    "# Função para criar estrutura com fator estrutural\n",
    "bd_estr_fator_estrutural = criar_estrutura_com_fator_estrutural(bd_estrutura_filtrada)\n",
    "\n",
    "print(\"✅ Geração de tabelas de dados LTP concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51074665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Primeiro Calculo NEC_PCS e Distribuição Capacidade concluídos!\n"
     ]
    }
   ],
   "source": [
    "########### TRAZER CAMPOS DA MATRIZ TRANSFERENCIA E ESTOQUE ORIGEM ###########\n",
    "bd_LTP_NEC = bd_prod_rot.copy()\n",
    "\n",
    "# Trazer para bd_produtos_nec as coluna de status da tabela bd_mat_transf_comp, colunas considera_estoq_origem e considera_estoq_triangulacao\n",
    "\n",
    " # Carregar matriz_transf_componentes\n",
    "bd_mat_transf_comp = pd.read_excel(\n",
    "    pasta_input / 'matriz_transf_componentes.xlsx',\n",
    "    sheet_name='matriz_transf_componentes',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "bd_mat_transf_comp['cod_produto'] = bd_mat_transf_comp['cod_produto'].astype(str).str.strip().str.upper()\n",
    "# Filtrar o DataFrame bd_mat_transf_comp para manter apenas as linhas onde 'validacao' == 'Sim'\n",
    "bd_mat_transf_comp = bd_mat_transf_comp[bd_mat_transf_comp['validacao'].str.upper() == 'SIM']\n",
    "bd_mat_transf_comp = bd_mat_transf_comp.reset_index(drop=True)\n",
    "\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_mat_transf_comp[['unid_destino', 'unid_origem', 'unid_triangulacao', 'cod_produto', 'considera_estoq_origem', 'considera_estoq_triangulacao']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_destino', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Trazer os valores de Estoque Inicial e Estoque Transferencia das unidades Origem\n",
    "# Cria uma cópia da tabela para servir como origem\n",
    "bd_produtos_origem = bd_LTP_NEC[['UNID_FAT', 'MES_REF', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS']].copy()\n",
    "\n",
    "# Remover duplicatas bd_produtos_origem\n",
    "bd_produtos_origem = bd_produtos_origem.drop_duplicates(subset=['UNID_FAT', 'MES_REF', 'COD_PROD']).reset_index(drop=True)\n",
    "\n",
    "bd_produtos_origem = bd_produtos_origem.rename(columns={\n",
    "    'UNID_FAT': 'UNID_FAT_ORIGEM',\n",
    "    'MES_REF': 'MES_REF_ORIGEM',\n",
    "    'COD_PROD': 'COD_PROD_ORIGEM',\n",
    "    'EST_INI_PCS': 'EST_INI_PCS_ORIGEM',\n",
    "    'EST_TRANS_PCS': 'EST_TRANS_PCS_ORIGEM'\n",
    "})\n",
    "\n",
    "# Eliminar duplicatas da tabela de origem\n",
    "bd_produtos_origem = bd_produtos_origem.drop_duplicates(subset=['UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM', 'EST_INI_PCS_ORIGEM', 'EST_TRANS_PCS_ORIGEM'])\n",
    "\n",
    "# Faz o merge na própria tabela, buscando os valores da origem\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_produtos_origem,\n",
    "    how='left',\n",
    "    left_on=['unid_origem', 'MES_REF', 'COD_PROD'],\n",
    "    right_on=['UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM']\n",
    ")\n",
    "\n",
    "# Criar uma tabela com cópia na bd_produtos_nec ter somente as colunas de interesse e buscar os valores de estoque origem e estoque triangulação\n",
    "bd_produtos_estoque_origem_triangulacao = bd_LTP_NEC\n",
    "colunas_excluir = ['considera_estoq_origem', 'considera_estoq_triangulacao']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "\n",
    "# Trazer para bd_produtos_nec as colunas de status da tabela bd_mat_transf_comp, colunas considera_estoq_origem e considera_estoq_triangulacao\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao.merge(\n",
    "    bd_mat_transf_comp[['unid_origem', 'cod_produto', 'considera_estoq_origem']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_origem', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Excluir as colunas unid_triangulacao e cod_produto\n",
    "colunas_excluir = ['unid_triangulacao', 'cod_produto']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao.merge(\n",
    "    bd_mat_transf_comp[['unid_triangulacao', 'cod_produto', 'considera_estoq_triangulacao']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_triangulacao', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Excluir as colunas unid_destino e cod_produto\n",
    "colunas_excluir = ['unid_triangulacao', 'cod_produto']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao[['MES_REF', 'UNID_FAT', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS','considera_estoq_origem', 'considera_estoq_triangulacao']]\n",
    "\n",
    "# Eliminar duplicatas da bd_produtos_estoque_origem_triangulacao\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao.drop_duplicates(subset=['MES_REF', 'UNID_FAT', 'COD_PROD']).reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Trazer os valores de Estoque Inicial e Estoque Transferencia das unidades Triangulação\n",
    "# Cria uma cópia da tabela para servir como Triangulação\n",
    "bd_produtos_triangulacao = bd_LTP_NEC[['UNID_FAT', 'MES_REF', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS']].copy()\n",
    "\n",
    "# Renomeando e organizando colunas bd_produtos_triangulacao\n",
    "bd_produtos_triangulacao = bd_produtos_triangulacao.rename(columns={\n",
    "    'UNID_FAT': 'UNID_FAT_TRIANG',\n",
    "    'MES_REF': 'MES_REF_TRIANG',\n",
    "    'COD_PROD': 'COD_PROD_TRIANG',\n",
    "    'EST_INI_PCS': 'EST_INI_PCS_TRIANG',\n",
    "    'EST_TRANS_PCS': 'EST_TRANS_PCS_TRIANG'\n",
    "})\n",
    "\n",
    "# Remover Duplicatas da tabela de triangulação\n",
    "bd_produtos_triangulacao = bd_produtos_triangulacao.drop_duplicates(subset=['UNID_FAT_TRIANG', 'MES_REF_TRIANG', 'COD_PROD_TRIANG']).reset_index(drop=True)\n",
    "\n",
    "# Faz o merge na própria tabela, buscando os valores da TRIANG\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_produtos_triangulacao,\n",
    "    how='left',\n",
    "    left_on=['unid_triangulacao', 'MES_REF', 'COD_PROD'],\n",
    "    right_on=['UNID_FAT_TRIANG', 'MES_REF_TRIANG', 'COD_PROD_TRIANG']\n",
    ")\n",
    "\n",
    "# Somar valores Estoque Inicial e Transferencia da Origem\n",
    "def ORI_TOT_PCS(row):\n",
    "    if str(row.get('considera_estoq_origem', '')).strip().upper() == 'SIM':\n",
    "        return row.get('EST_INI_PCS_ORIGEM', 0) + row.get('EST_TRANS_PCS_ORIGEM', 0)\n",
    "    else:\n",
    "        return 0\n",
    "bd_LTP_NEC['ORI_TOT_PCS'] = bd_LTP_NEC.apply(ORI_TOT_PCS, axis=1)\n",
    "\n",
    "# Somar valores Estoque Inicial e Transferencia da Triangulação\n",
    "def TRIANG_TOT_PCS(row):\n",
    "    if str(row.get('considera_estoq_triangulacao', '')).strip().upper() == 'SIM':\n",
    "        return row.get('EST_INI_PCS_TRIANG', 0) + row.get('EST_TRANS_PCS_TRIANG', 0)\n",
    "    else:\n",
    "        return 0\n",
    "bd_LTP_NEC['TRIANG_TOT_PCS'] = bd_LTP_NEC.apply(TRIANG_TOT_PCS, axis=1)\n",
    "\n",
    "# Eliminar colunas desnecessárias da bd_LTP_NEC\n",
    "colunas_excluir = [\n",
    "    'unid_destino', 'unid_origem', 'unid_triangulacao', 'cod_produto',\n",
    "    'considera_estoq_origem', 'considera_estoq_triangulacao',\n",
    "    'UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM',\n",
    "    'EST_INI_PCS_ORIGEM', 'EST_TRANS_PCS_ORIGEM',\n",
    "    'UNID_FAT_TRIANG', 'MES_REF_TRIANG', 'COD_PROD_TRIANG',\n",
    "    'EST_INI_PCS_TRIANG', 'EST_TRANS_PCS_TRIANG'\n",
    "]\n",
    "bd_LTP_NEC = drop_colunas(bd_LTP_NEC, colunas_excluir)\n",
    "\n",
    "# Aplicar zero nos campos vazios de NEC_PCS, ORI_TOT_PCS e TRIANG_TOT_PCS\n",
    "colunas_preencher = ['ORI_TOT_PCS', 'TRIANG_TOT_PCS']\n",
    "bd_LTP_NEC[colunas_preencher] = bd_LTP_NEC[colunas_preencher].fillna(0).astype(float)\n",
    "\n",
    "# Filtrar bd_LTP_NEC pelo campo MAT_PAR_prioridade > 0\n",
    "bd_LTP_NEC = bd_LTP_NEC[bd_LTP_NEC['PRIOR_MATPAR'] > 0].reset_index(drop=True)\n",
    "\n",
    "#*****************************************# ID_ULT_PRIORI #*****************************************\n",
    "# Criar a tabela bd_Ultimo_Roteiro_MAT_PAR com os campos ID_PROD_UNID_FAT, PRIOR_MATPAR e PRIOR_ROT, eliminando as duplicatas\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_LTP_NEC[['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT']]\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_Ultimo_Roteiro_MAT_PAR.sort_values(\n",
    "    by=['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT'],\n",
    "    ascending=[True, True, False]\n",
    ").reset_index(drop=True)\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_Ultimo_Roteiro_MAT_PAR.drop_duplicates(subset='ID_PROD_UNID_FAT', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Criar a coluna ID_ULT_PRIORI fazendo o merge entre bd_LTP_NEC e bd_Ultimo_Roteiro_MAT_PAR para trazer dados a coluna ID_PROD_UNID_FAT\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_Ultimo_Roteiro_MAT_PAR[['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT']].rename(columns={'ID_PROD_UNID_FAT': 'ID_ULT_PRIORI'}),\n",
    "    how='left',\n",
    "    left_on=['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT'],\n",
    "    right_on=['ID_ULT_PRIORI', 'PRIOR_MATPAR', 'PRIOR_ROT']\n",
    ")\n",
    "\n",
    "bd_LTP_NEC = bd_LTP_NEC.sort_values(['IND']).reset_index(drop=True)\n",
    "\n",
    "# Zerar valores campo SALDO_PREV_PROX_MES_PCS, considerando se MESMA_REG for igual a \"SIM\", manter valor, se MESMA_REG for \"NAO\", zerar o valor\n",
    "bd_LTP_NEC['SALDO_PREV_PROX_MES_PCS'] = np.where(\n",
    "    bd_LTP_NEC['MESMA_REG'] == 'NAO',\n",
    "    bd_LTP_NEC['SALDO_PREV_PROX_MES_PCS'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Criando cópia dos campos e adicinar nos rótulos, no incio do nome LTP\n",
    "campos_copiar = [\n",
    "    'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'SALDO_PREV_PCS', 'EST_TRANS_PCS','SALDO_PREV_PROX_MES_PCS'\n",
    "]\n",
    "\n",
    "for col in campos_copiar:\n",
    "    bd_LTP_NEC['LTP_' + col] = bd_LTP_NEC[col]\n",
    "    \n",
    "# Adicionar coluna LTP_COMP_NEC_PCS com valor 0\n",
    "bd_LTP_NEC['LTP_COMP_NEC_PCS'] = 0\n",
    "\n",
    "# Utilizando Flags Parametros Lote Minimo e Multiplo Embalagens e Calculando os campos VAR_NEC1, VAR_NEC2 e VAR_NEC3\n",
    "def carregar_flags_ltp(pasta_painel):\n",
    "    def carregar_planilha_segura(caminho: Path, **kwargs):\n",
    "        try:\n",
    "            return load_workbook(caminho, **kwargs)\n",
    "        except PermissionError:\n",
    "            with tempfile.NamedTemporaryFile(suffix=caminho.suffix, delete=False) as tmp:\n",
    "                shutil.copy2(caminho, tmp.name)\n",
    "                return load_workbook(tmp.name, **kwargs)\n",
    "\n",
    "    def obter_valor_nome_definido(wb, nome_definido):\n",
    "        nome_planilha, intervalo = next(nome_definido.destinations)\n",
    "        planilha = wb[nome_planilha]\n",
    "        coluna_min, linha_min, _, _ = range_boundaries(intervalo)\n",
    "        return planilha.cell(linha_min, coluna_min).value\n",
    "\n",
    "    arquivo_painel = pasta_painel / 'Painel_LTP.xlsm'\n",
    "    planilha_ltp = carregar_planilha_segura(arquivo_painel, data_only=True)\n",
    "    nome_lote_min = planilha_ltp.defined_names['FlagLoteMinimo']\n",
    "    nome_multiplo_emb = planilha_ltp.defined_names['FlagMultiploEmb']\n",
    "    lote_min = obter_valor_nome_definido(planilha_ltp, nome_lote_min)\n",
    "    multiplo_emb = obter_valor_nome_definido(planilha_ltp, nome_multiplo_emb)\n",
    "    return str(lote_min).strip().upper(), str(multiplo_emb).strip().upper()\n",
    "\n",
    "lote_min_flag, multiplo_emb_flag = carregar_flags_ltp(pasta_painel)\n",
    "\n",
    "# Mover Colunas ID_RECURSO e id_FERRAMENTA para o final do DataFrame, depois da coluna LTP_COMP_NEC_PCS\n",
    "bd_LTP_NEC = bd_LTP_NEC[\n",
    "    [col for col in bd_LTP_NEC.columns if col not in ['ID_RECURSO', 'ID_FERRAMENTA']] +\n",
    "    ['ID_RECURSO', 'ID_FERRAMENTA']\n",
    "]\n",
    "\n",
    "# Mover colunas HOR_REC, HOR_FER, HOR_CAP para o final do DataFrame, depois da coluna LTP_COMP_NEC_PCS\n",
    "bd_LTP_NEC = bd_LTP_NEC[\n",
    "    [col for col in bd_LTP_NEC.columns if col not in ['HOR_REC', 'HOR_FER']] +\n",
    "    ['HOR_REC', 'HOR_FER']\n",
    "]\n",
    "\n",
    "# Excluir coluna HOR_CAP\n",
    "bd_LTP_NEC = drop_colunas(bd_LTP_NEC, ['HOR_CAP'])\n",
    "\n",
    "# criar coluna COMP com valor 'SIM' ou 'NAO' dependendo se o COD_PROD está na tabela bd_estruturas\n",
    "insumos = set(bd_estruturas['cod_insumo'].astype(str).str.strip().str.upper())\n",
    "bd_LTP_NEC['COMP'] = (bd_LTP_NEC['COD_PROD'].astype(str).str.strip().str.upper().isin(insumos).map({True: 'SIM', False: 'NAO'}))\n",
    "\n",
    "print(\"✅ Primeiro Calculo NEC_PCS e Distribuição Capacidade concluídos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039421a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar Data\n",
    "bd_LTP_NEC['MES_REF'] = bd_LTP_NEC['MES_REF'].dt.normalize()\n",
    "\n",
    "# Criar uma lista de MES_REF únicos\n",
    "mes_refs = bd_LTP_NEC['MES_REF'].unique().tolist()\n",
    "\n",
    "# Loop sobre os meses (no momento apenas o primeiro é tratado com cálculo)\n",
    "for mes_ref in mes_refs:\n",
    "    # Filtrar bd_LTP_M1 por mes_ref\n",
    "    if mes_ref == mes_refs[0]:\n",
    "        bd_LTP_M1 = bd_LTP_NEC[bd_LTP_NEC['MES_REF'] == mes_ref].reset_index(drop=True)\n",
    "        bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "        # # *************************# Explosão da Estrutura e necessidade de componentes #**************************\n",
    "        bd_ltp_estrutura_explodida = explodir_estrutura_ltp(bd_estrutura_filtrada, bd_LTP_M1)\n",
    "        bd_nec_comp_expl= calcular_explosao_necessidades(bd_ltp_estrutura_explodida, bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1 = atualizar_ltp_comp_nec_pcs(bd_LTP_M1, bd_nec_comp_expl)\n",
    "        # *********************# Recalculando NEC_PCS e demais campos, após explosão componentes #*******************************\n",
    "        bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "        # **************************************# Iniciando Cortes Recursos #****************************************************\n",
    "        bd_mat_cortes = cria_bd_mat_cortes_REC(bd_LTP_M1)\n",
    "        bd_LTP_M1 = calcular_decomposicao_nec_pcs_para_precisao_corte(bd_LTP_M1)\n",
    "        \n",
    "        # FIXME: FILTRAR bd_mat_cortes campo ALOC_REC\n",
    "        bd_mat_cortes = bd_mat_cortes[bd_mat_cortes['ALOC_REC'] == '00618'].reset_index(drop=True)\n",
    "        \n",
    "        # Iniciar lopp de cortes para RECURSOS\n",
    "        for i in range(len(bd_mat_cortes)):\n",
    "            aloc_rec = bd_mat_cortes.at[i, 'ALOC_REC']\n",
    "            unid_prod = bd_mat_cortes.at[i, 'UNID_PROD']\n",
    "\n",
    "            # Filtrar bd_LTP para o recurso atual\n",
    "            filtro = (bd_LTP_M1['ALOC_REC'] == aloc_rec) & (bd_LTP_M1['UNID_PROD'] == unid_prod)\n",
    "            bd_LTP_filtrado_PI = bd_LTP_M1.loc[filtro].copy()\n",
    "            \n",
    "            # Somar NEC_PCS com TIPO_PROD = 'PI', para validar se entrar no tratamento de PI ou se segue para tratamento de PA\n",
    "            soma_nec_pcs_pi = bd_LTP_filtrado_PI[bd_LTP_filtrado_PI['TIPO_PROD'] == 'PI']['NEC_PCS'].sum()\n",
    "            \n",
    "            # Verificar se em bd_LTP_filtrado existe TIPO_PROD = PI, e se soma_nec_pcs_pi > 0\n",
    "            if 'PI' in bd_LTP_filtrado_PI['TIPO_PROD'].values and soma_nec_pcs_pi > 0:\n",
    "                # Listar os COD_PROD e UNID_PROD que sejam TIPO_PROD = \"PI\", eliminando duplicatas, eliminando NEC_PCS = 0\n",
    "                pi_itens = bd_LTP_filtrado_PI[bd_LTP_filtrado_PI['TIPO_PROD'] == 'PI'][['COD_PROD', 'UNID_PROD', 'PCS_HORA', 'NEC_PCS']].drop_duplicates().reset_index(drop=True)\n",
    "                pi_itens = pi_itens[pi_itens['NEC_PCS'] > 0].reset_index(drop=True)\n",
    "                pi_itens['RASTREABILIDADE'] = pi_itens['COD_PROD'] + '|' + pi_itens['UNID_PROD']\n",
    "                pi_itens['COD_INSUMO'] = pi_itens['COD_PROD']\n",
    "                # Filtrar COD_PROD e UNID_PROD do pi_itens, nos campos COD_INSUMO e UNID_PROD da bd_ltp_estrutura_explodida\n",
    "                lista_itens_pa_estr_expl = bd_ltp_estrutura_explodida[\n",
    "                    (bd_ltp_estrutura_explodida[['COD_INSUMO', 'UNID_PROD']].apply(tuple, axis=1).isin(\n",
    "                        pi_itens[['COD_PROD', 'UNID_PROD']].apply(tuple, axis=1)\n",
    "                    ))][['COD_PROD', 'UNID_PROD']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "                # Filtrar cada COD_PROD e UNID_PROD da lista_itens_pa_estr_expl na bd_LTP_filtrado_PI, na bd_LTP_M1, trazendo as linhas encontradas incoporporando em bd_LTP_filtrado_PI, mantendo o índice original \n",
    "                bd_LTP_filtrado_PI_PA = bd_LTP_filtrado_PI.copy()\n",
    "                \n",
    "                for _, pi in pi_itens.iterrows():\n",
    "                    cod_pi = pi['COD_PROD']\n",
    "                    unid_pi = pi['UNID_PROD']\n",
    "                    rastreabilidade = pi['RASTREABILIDADE']\n",
    "                    pcs_hora_pi = pi['PCS_HORA']\n",
    "                    cod_insumo = pi['COD_INSUMO']\n",
    "\n",
    "                    # PAs originados por esse PI na estrutura\n",
    "                    lista_itens_pa_estr_expl = bd_ltp_estrutura_explodida[\n",
    "                        (bd_ltp_estrutura_explodida['COD_INSUMO'] == cod_pi) &\n",
    "                        (bd_ltp_estrutura_explodida['UNID_PROD'] == unid_pi)\n",
    "                    ][['COD_PROD', 'UNID_PROD']].drop_duplicates()\n",
    "\n",
    "                    for _, pa in lista_itens_pa_estr_expl.iterrows():\n",
    "                        cod_pa = pa['COD_PROD']\n",
    "                        unid_prod_estrutura = pa['UNID_PROD']\n",
    "\n",
    "                        filtro_estrutura = (\n",
    "                            (bd_LTP_M1['COD_PROD'] == cod_pa) &\n",
    "                            (bd_LTP_M1['UNID_PROD'] == unid_prod_estrutura)\n",
    "                        )\n",
    "\n",
    "                        linhas_filtradas_pa_LTP = bd_LTP_M1.loc[filtro_estrutura].copy()\n",
    "\n",
    "                        linhas_filtradas_pa_LTP['RASTREABILIDADE'] = rastreabilidade\n",
    "                        linhas_filtradas_pa_LTP['PCS_HORA_PI'] = pcs_hora_pi\n",
    "                        linhas_filtradas_pa_LTP['COD_INSUMO'] = cod_insumo\n",
    "\n",
    "                        bd_LTP_filtrado_PI_PA = pd.concat(\n",
    "                            [bd_LTP_filtrado_PI_PA, linhas_filtradas_pa_LTP],\n",
    "                            ignore_index=False\n",
    "                        )\n",
    "                        \n",
    "                # Trazer para bd_LTP_filtrado a coluna FATOR_ESTRUTURAL da bd_estr_fator_estrutural, pelas colunas COD_PROD e UNID_PROD e COD_INSUMO\n",
    "                bd_LTP_filtrado_PI_PA = bd_LTP_filtrado_PI_PA.merge(\n",
    "                    bd_estr_fator_estrutural[['COD_PROD', 'UNID_PROD', 'COD_INSUMO', 'FATOR_ESTRUTURAL']],\n",
    "                    how='left',\n",
    "                    left_on=['COD_PROD', 'UNID_PROD', 'COD_INSUMO'],\n",
    "                    right_on=['COD_PROD', 'UNID_PROD', 'COD_INSUMO']\n",
    "                )\n",
    "                \n",
    "                # Criar coluna NEC_ATEND_PCS_PI, calculando NEC_ATEND_PCS multiplicando por FATOR_ESTRUTURAL\n",
    "                bd_LTP_filtrado_PI_PA['NEC_ATEND_PCS_PI'] = bd_LTP_filtrado_PI_PA['NEC_ATEND_PCS'] * bd_LTP_filtrado_PI_PA['FATOR_ESTRUTURAL']\n",
    "                \n",
    "                # Criar coluna NEC_ATEND_HR_PI, calculando NEC_ATEND_PCS_PI dividindo por PCS_HORA_PI\n",
    "                bd_LTP_filtrado_PI_PA['NEC_ATEND_HR_PI'] = bd_LTP_filtrado_PI_PA['NEC_ATEND_PCS_PI'] / bd_LTP_filtrado_PI_PA['PCS_HORA_PI']\n",
    "                \n",
    "                # ----------------------------------------- Realizar Rateio ---------------------------------------------\n",
    "                # Distribuir NEC_ATEND_HR_PI nas colunas C_ARR_HR, C_AT_HR, PV_HR, PV_PROX_HR, ES_HR, DIF_LM_HR, DIF_EMB_HR proporcionalmente\n",
    "                cols_rateio = ['C_ARR_HR','C_AT_HR','PV_HR','PV_PROX_HR','ES_HR','DIF_LM_HR','DIF_EMB_HR']\n",
    "\n",
    "                # 1. Copiar base original\n",
    "                base = bd_LTP_filtrado_PI_PA[cols_rateio].copy()\n",
    "\n",
    "                # 2. Calcular base de rateio\n",
    "                base_sum = base.sum(axis=1)\n",
    "\n",
    "                # 3. Evitar divisão por zero\n",
    "                fator = bd_LTP_filtrado_PI_PA['NEC_ATEND_HR_PI'].where(base_sum > 0, 0)\n",
    "\n",
    "                # 4. Rateio e sobreescrever as colunas originais com os valores rateados\n",
    "                bd_LTP_filtrado_PI_PA[cols_rateio] = base.div(base_sum, axis=0).mul(fator, axis=0).fillna(0)\n",
    "                # --------------------------------------- Fim Realizar Rateio -------------------------------------------         \n",
    "                \n",
    "                # Função que cria a matriz de cortes, e sequencia de cortes que deve ser respeitada\n",
    "                bd_matriz_cortes_rec = matriz_cortes_horas(bd_LTP_filtrado_PI_PA)\n",
    "                \n",
    "                # Filtrar bd_mat_cortes pelas variaveis unid_prod e aloc_rec, para trazer somente os cortes relacionados com o recurso e produto do PI, e criar um ecossistema somente para o PI definido e tratado neste momento\n",
    "                bd_mat_cortes_horas_PI_analise = bd_mat_cortes[\n",
    "                    (bd_mat_cortes['UNID_PROD'] == unid_prod) &\n",
    "                    (bd_mat_cortes['ALOC_REC'] == aloc_rec)\n",
    "                ]\n",
    "                \n",
    "                # Somar horas totais de Corte\n",
    "                horas_para_corte = bd_mat_cortes_horas_PI_analise['CORTE_HR'].sum()\n",
    "                \n",
    "                # Identifica as colunas da matriz que representam combinações no formato:\n",
    "                # \"MESMA_REG|NOME_COLUNA_HR\"\n",
    "                # Exemplo: \"NAO|PV_PROX_HR\"\n",
    "                # Identifica colunas da matriz no formato \"MESMA_REG|COLUNA_HR\"\n",
    "                # Essas colunas representam a sequência lógica de corte.\n",
    "                cols_matriz = [c for c in bd_matriz_cortes_rec.columns if '|' in c]\n",
    "\n",
    "                for col_matriz in cols_matriz:\n",
    "\n",
    "                    # Se já não houver mais horas a cortar, interrompe o loop.\n",
    "                    # O corte deve ocorrer até zerar horas_para_corte.\n",
    "                    if horas_para_corte <= 0:\n",
    "                        break\n",
    "\n",
    "                    # Total agregado disponível nesta coluna da matriz.\n",
    "                    # A matriz é utilizada apenas como referência de decisão (nível consolidado).\n",
    "                    total_col = float(bd_matriz_cortes_rec[col_matriz].sum())\n",
    "\n",
    "                    # Se não há horas disponíveis nesta combinação (SIM/NAO + coluna HR),\n",
    "                    # ignora e segue para a próxima coluna.\n",
    "                    if total_col <= 0:\n",
    "                        continue\n",
    "\n",
    "                    # Define quanto será efetivamente cortado nesta coluna.\n",
    "                    # Se a coluna cobre tudo que falta cortar, corta apenas o necessário.\n",
    "                    # Caso contrário, consome toda a coluna e continuará para a próxima.\n",
    "                    corte_horas_col = min(horas_para_corte, total_col)\n",
    "\n",
    "                    # Separa o cabeçalho da matriz:\n",
    "                    # Exemplo: \"NAO|PV_PROX_HR\"\n",
    "                    # mesma_reg_alvo -> 'NAO'\n",
    "                    # col_hr_alvo    -> 'PV_PROX_HR'\n",
    "                    mesma_reg_alvo, col_hr_alvo = col_matriz.split('|', 1)\n",
    "\n",
    "                    # Define nomes das colunas auxiliares criadas no ecossistema detalhado.\n",
    "                    col_rateio     = f\"{col_hr_alvo}_RATEIO\"\n",
    "                    col_corte      = f\"{col_hr_alvo}_CORTE\"\n",
    "                    col_saldo_hr   = f\"{col_hr_alvo}_SALDO_HR\"\n",
    "                    col_saldo_pcs  = f\"{col_hr_alvo}_SALDO_PCS\"\n",
    "\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    # Calcula percentual de participação da linha no total da coluna analisada.\n",
    "                    # Aplica apenas às linhas cujo MESMA_REG coincide com o alvo.\n",
    "                    # Regras:\n",
    "                    # - rateio = HR_linha / total_col (apenas MESMA_REG alvo)\n",
    "                    # - fora do alvo, rateio = 0\n",
    "                    # - proteção contra divisão por zero já garantida em total_col <= 0 (continue)\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    bd_LTP_filtrado_PI_PA[col_rateio] = np.where(\n",
    "                        bd_LTP_filtrado_PI_PA['MESMA_REG'] == mesma_reg_alvo,\n",
    "                        bd_LTP_filtrado_PI_PA[col_hr_alvo] / total_col,\n",
    "                        0\n",
    "                    )\n",
    "\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    # Calcula horas cortadas por linha usando apenas o montante\n",
    "                    # efetivamente cortado nesta coluna (corte_horas_col).\n",
    "                    # Regras:\n",
    "                    # - CORTE_HR_linha = rateio * corte_horas_col\n",
    "                    # - CORTE_HR nunca deve ultrapassar o HR disponível da linha\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    bd_LTP_filtrado_PI_PA[col_corte] = bd_LTP_filtrado_PI_PA[col_rateio] * corte_horas_col\n",
    "                    bd_LTP_filtrado_PI_PA[col_corte] = np.minimum(\n",
    "                        bd_LTP_filtrado_PI_PA[col_corte],\n",
    "                        bd_LTP_filtrado_PI_PA[col_hr_alvo]\n",
    "                    )\n",
    "\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    # Saldo remanescente pós-corte em HR\n",
    "                    # Mantém coerência com o restante do modelo:\n",
    "                    # - saldo_hr = hr_original - corte_hr\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    bd_LTP_filtrado_PI_PA[col_saldo_hr] = bd_LTP_filtrado_PI_PA[col_hr_alvo] - bd_LTP_filtrado_PI_PA[col_corte]\n",
    "                    bd_LTP_filtrado_PI_PA[col_saldo_hr] = bd_LTP_filtrado_PI_PA[col_saldo_hr].clip(lower=0)\n",
    "\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    # Conversão do saldo remanescente de HR para PCS\n",
    "                    # Mantém coerência com o restante do modelo (PCS = HR * PCS_HORA)\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    bd_LTP_filtrado_PI_PA[col_saldo_pcs] = bd_LTP_filtrado_PI_PA[col_saldo_hr] * bd_LTP_filtrado_PI_PA['PCS_HORA']\n",
    "\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    # Identificação da coluna estrutural LTP correspondente à coluna HR\n",
    "                    # O mapeamento será usado na etapa seguinte para atualizar a base LTP.\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    if col_hr_alvo == \"C_ARR_HR\":\n",
    "                        col_ltp_alvo = \"LTP_CART_ARR_MES_ANT\"\n",
    "                    elif col_hr_alvo == \"C_AT_HR\":\n",
    "                        col_ltp_alvo = \"LTP_CART_MES_ATUAL\"\n",
    "                    elif col_hr_alvo == \"PV_HR\":\n",
    "                        col_ltp_alvo = \"LTP_SALDO_PREV_PCS\"\n",
    "                    elif col_hr_alvo == \"PV_PROX_HR\":\n",
    "                        col_ltp_alvo = \"LTP_SALDO_PREV_PROX_MES_PCS\"\n",
    "                    elif col_hr_alvo == \"ES_HR\":\n",
    "                        col_ltp_alvo = \"LTP_EST_SEG_PCS\"\n",
    "                    else:\n",
    "                        col_ltp_alvo = None\n",
    "\n",
    "\t\t\t\t\t# ----------------------------------------------------------------------\n",
    "\t\t\t\t\t# Atualização da coluna estrutural LTP correspondente\n",
    "\t\t\t\t\t# A coluna LTP_*_PCS passa a assumir o saldo remanescente pós-corte\n",
    "\t\t\t\t\t# calculado anteriormente em col_saldo_pcs.\n",
    "\t\t\t\t\t# A atualização ocorre apenas nas linhas que participaram do rateio.\n",
    "\t\t\t\t\t# ----------------------------------------------------------------------\n",
    "                    if col_ltp_alvo is not None:\n",
    "                        mask_alt = bd_LTP_filtrado_PI_PA[col_rateio] > 0\n",
    "\n",
    "                        if mask_alt.any():\n",
    "                            bd_LTP_filtrado_PI_PA.loc[mask_alt, col_ltp_alvo] = \\\n",
    "                                bd_LTP_filtrado_PI_PA.loc[mask_alt, col_saldo_pcs]\n",
    "\n",
    "                            # ----------------------------------------------------------------------\n",
    "                            # Propaga a atualização para bd_LTP_M1 usando IND\n",
    "                            # Só atualiza as linhas alteradas (mask_alt) e só a coluna LTP alvo.\n",
    "                            # Observação:\n",
    "                            # - Usa mapeamento 1:1 por IND. Se houver IND repetido no ecossistema,\n",
    "                            #   o último valor em vals_alt prevalece.\n",
    "                            # ----------------------------------------------------------------------\n",
    "                            inds_alt = bd_LTP_filtrado_PI_PA.loc[mask_alt, \"IND\"].values\n",
    "                            vals_alt = bd_LTP_filtrado_PI_PA.loc[mask_alt, col_ltp_alvo].values\n",
    "\n",
    "                            bd_LTP_M1.loc[bd_LTP_M1[\"IND\"].isin(inds_alt), col_ltp_alvo] = vals_alt\n",
    "\n",
    "                    # Atualiza o saldo restante a cortar (TEM que ficar no final da iteração).\n",
    "                    horas_para_corte -= corte_horas_col\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                # Pós-corte do PI:\n",
    "                # Após atualizar colunas LTP_* na bd_LTP_M1, é necessário recalcular o modelo para refletir:\n",
    "                # - NOVA NECESSIDADE (NEC_PCS / HR)\n",
    "                # - NOVA CAPACIDADE (distribuição)\n",
    "                # - NOVOS CAMPOS DERIVADOS (atendimento, estouro, etc.)\n",
    "                # - NOVA NECESSIDADE DE COMPONENTES (quando PAs foram cortados em detrimento do PI)\n",
    "                # - NOVA DECOMPOSIÇÃO (C_ARR/C_AT/PV/...) para próxima decisão de corte\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                # Pós-corte do PI:\n",
    "                # Após atualizar colunas LTP_* na bd_LTP_M1, é necessário recalcular o modelo para refletir o novo cenário\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "                bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "                bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "\n",
    "                # ******************# Explosão da Estrutura e necessidade de componentes (pós-corte) \n",
    "                bd_ltp_estrutura_explodida = explodir_estrutura_ltp(bd_estrutura_filtrada, bd_LTP_M1)\n",
    "                bd_nec_comp_expl = calcular_explosao_necessidades(bd_ltp_estrutura_explodida, bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "                bd_LTP_M1 = atualizar_ltp_comp_nec_pcs(bd_LTP_M1, bd_nec_comp_expl)\n",
    "\n",
    "                # ******************# Recalculando NEC_PCS e demais campos, após explosão componentes (pós-corte)\n",
    "                bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "                bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "                bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "                # ******************# Recalcular decomposição para próxima decisão de corte \n",
    "                bd_LTP_M1 = calcular_decomposicao_nec_pcs_para_precisao_corte(bd_LTP_M1)\n",
    "                bd_mat_cortes = cria_bd_mat_cortes_REC(bd_LTP_M1)         \n",
    "\n",
    "                \n",
    "            else:\n",
    "                # Aplicar cortes pois no filtro máquina temos somente PAs\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a792d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNID_FAT</th>\n",
       "      <th>UNID_PROD</th>\n",
       "      <th>MESMA_REG</th>\n",
       "      <th>PRIOR_MATPAR</th>\n",
       "      <th>PRIOR_ROT</th>\n",
       "      <th>ALOC_REC</th>\n",
       "      <th>LTP_EST_SEG_PCS</th>\n",
       "      <th>LTP_EST_INI_PCS</th>\n",
       "      <th>LTP_CART_ARR_MES_ANT</th>\n",
       "      <th>LTP_CART_MES_ATUAL</th>\n",
       "      <th>LTP_SALDO_PREV_PCS</th>\n",
       "      <th>LTP_SALDO_PREV_PROX_MES_PCS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67022</td>\n",
       "      <td>41792.00</td>\n",
       "      <td>41792.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>30367.00</td>\n",
       "      <td>71871.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67014</td>\n",
       "      <td>18846.00</td>\n",
       "      <td>18846.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>15294.00</td>\n",
       "      <td>50063.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67014</td>\n",
       "      <td>23260.00</td>\n",
       "      <td>23260.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21231.00</td>\n",
       "      <td>52805.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67014</td>\n",
       "      <td>15417.00</td>\n",
       "      <td>15673.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>10831.00</td>\n",
       "      <td>11304.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67014</td>\n",
       "      <td>33384.00</td>\n",
       "      <td>33384.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>550.00</td>\n",
       "      <td>24496.00</td>\n",
       "      <td>51824.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>Matriz</td>\n",
       "      <td>NAO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67022</td>\n",
       "      <td>14639.00</td>\n",
       "      <td>14639.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>7392.00</td>\n",
       "      <td>1891.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UNID_FAT UNID_PROD MESMA_REG  PRIOR_MATPAR  PRIOR_ROT ALOC_REC  LTP_EST_SEG_PCS  LTP_EST_INI_PCS  LTP_CART_ARR_MES_ANT  LTP_CART_MES_ATUAL  LTP_SALDO_PREV_PCS  LTP_SALDO_PREV_PROX_MES_PCS\n",
       "0  Nordeste    Matriz       NAO             1          1    67022         41792.00         41792.00                  0.00               96.00            30367.00                     71871.58\n",
       "1  Nordeste    Matriz       NAO             1          1    67014         18846.00         18846.00                  0.00              180.00            15294.00                     50063.10\n",
       "2  Nordeste    Matriz       NAO             1          1    67014         23260.00         23260.00                  0.00                0.00            21231.00                     52805.59\n",
       "3  Nordeste    Matriz       NAO             1          1    67014         15417.00         15673.00                  0.00               80.00            10831.00                     11304.70\n",
       "4  Nordeste    Matriz       NAO             1          1    67014         33384.00         33384.00                  0.00              550.00            24496.00                     51824.19\n",
       "5  Nordeste    Matriz       NAO             1          1    67022         14639.00         14639.00                  0.00              200.00             7392.00                      1891.66"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar um df sobre a bd_LTP_M1, com a coluna IND = 3423, 3425, 3451, 3453, 3480, 3482, e mostrando somente as colunas UNID_FAT\tUNID_PROD\tMESMA_REG\tPRIOR_MATPAR\tPRIOR_ROT ALOC_REC, e as colunas LTP_EST_SEG_PCS\tLTP_EST_INI_PCS\tLTP_CART_ARR_MES_ANT\tLTP_CART_MES_ATUAL\tLTP_SALDO_PREV_PC\tLTP_SALDO_PREV_PROX_MES_PCS\n",
    "bd_LTP_M1_filtrado = bd_LTP_M1[bd_LTP_M1['IND'].isin([3423, 3425, 3451, 3453, 3480, 3482])][\n",
    "    ['UNID_FAT', 'UNID_PROD', 'MESMA_REG', 'PRIOR_MATPAR', 'PRIOR_ROT', 'ALOC_REC', 'LTP_EST_SEG_PCS', 'LTP_EST_INI_PCS', 'LTP_CART_ARR_MES_ANT', 'LTP_CART_MES_ATUAL', 'LTP_SALDO_PREV_PCS', 'LTP_SALDO_PREV_PROX_MES_PCS']\n",
    "].reset_index(drop=True)\n",
    "\n",
    "bd_LTP_M1_filtrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "169ca81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNID_PROD</th>\n",
       "      <th>ALOC_REC</th>\n",
       "      <th>HOR_REC</th>\n",
       "      <th>NEC_ESTOURO_HR_REC</th>\n",
       "      <th>NEC_ATEND_HR</th>\n",
       "      <th>%_OCUP_REC</th>\n",
       "      <th>CORTE_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matriz</td>\n",
       "      <td>00618</td>\n",
       "      <td>511.52</td>\n",
       "      <td>437.12</td>\n",
       "      <td>511.52</td>\n",
       "      <td>185.46</td>\n",
       "      <td>437.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UNID_PROD ALOC_REC  HOR_REC  NEC_ESTOURO_HR_REC  NEC_ATEND_HR  %_OCUP_REC  CORTE_HR\n",
       "0    Matriz    00618   511.52              437.12        511.52      185.46    437.12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_mat_cortes = bd_mat_cortes[bd_mat_cortes['ALOC_REC'] == '00618'].reset_index(drop=True)\n",
    "bd_mat_cortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917edc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_LTP_M1.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_mat_cortes.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_estrutura_filtrada.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_nec_comp_expl.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_LTP_filtrado_PI_PA.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_estr_fator_estrutural.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_matriz_cortes_rec.xlsx\n",
      "Tempo total de processamento: 0 min 33.8 s\n",
      "🎯 Processo concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Exportar para Excel\n",
    "\n",
    "# Listar os dataframes e seus nomes correspondentes\n",
    "dataframes_para_exportar = {\n",
    "    \"bd_LTP_M1\": bd_LTP_M1,\n",
    "    \"bd_mat_cortes\": bd_mat_cortes,\n",
    "    \"bd_estrutura_filtrada\": bd_estrutura_filtrada,\n",
    "    \"bd_nec_comp_expl\": bd_nec_comp_expl,\n",
    "    \"bd_LTP_filtrado_PI_PA\": bd_LTP_filtrado_PI_PA,\n",
    "    \"bd_estr_fator_estrutural\": bd_estr_fator_estrutural,\n",
    "    \"bd_matriz_cortes_rec\": bd_matriz_cortes_rec\n",
    "}\n",
    "\n",
    "for nome in dataframes_para_exportar:\n",
    "    df = globals()[nome]   # pega o objeto DataFrame pelo nome da variável\n",
    "    caminho_arquivo = pasta_output / f\"{nome}.xlsx\"\n",
    "    df.to_excel(caminho_arquivo, index=False)\n",
    "    print(f\"✅ Exportado: {caminho_arquivo}\")\n",
    "    \n",
    "timer.finalizar()\n",
    "print(\"🎯 Processo concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
