{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544ef674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mapeamento de pastas e importação de tabelas concluído!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from functions import *\n",
    "from FUNCTIONS_LTP import *\n",
    "import locale\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import range_boundaries\n",
    "import tempfile\n",
    "import shutil\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'Portuguese_Brazil.1252')  # Para Windows\n",
    "timer = Temporizador()\n",
    "timer.iniciar()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Aumenta o limite de largura da coluna para exibição\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Detecta se o script está sendo executado de um .py ou de um notebook\n",
    "try:\n",
    "    caminho_base = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # __file__ não existe em Jupyter ou ambiente interativo\n",
    "    caminho_base = Path.cwd()\n",
    "    \n",
    "pasta_input = caminho_base.parent / '01_INPUT'\n",
    "pasta_output = caminho_base.parent / '02_OUTPUT'\n",
    "pasta_painel = caminho_base.parent / '03_EXCEL'\n",
    "\n",
    "# Carregar bd_mat_param uma vez\n",
    "bd_mat_param = pd.read_excel(\n",
    "    pasta_input / 'matriz_parametros.xlsx',\n",
    "    sheet_name='matriz_parametros',\n",
    "    engine='calamine',\n",
    "    dtype={'produto': str}\n",
    ")\n",
    "bd_mat_param['produto'] = bd_mat_param['produto'].astype(str).str.strip().str.upper()\n",
    "produtos_param = set(bd_mat_param['produto'])\n",
    "colunas_manter = [\n",
    "    'produto', 'unidade_fat', 'unidade_prod', 'tipo_abast', 'prioridade'\n",
    "]\n",
    "\n",
    "bd_mat_param = bd_mat_param[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Carregar base_dados_produtos\n",
    "bd_prod = pd.read_excel(\n",
    "    pasta_input / 'base_dados_produtos.xlsx',\n",
    "    sheet_name='base_dados_produtos',\n",
    "    engine='calamine',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "bd_prod['cod_produto'] = bd_prod['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_prod['cod_produto'] = bd_prod['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_prod = bd_prod[bd_prod['cod_produto'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "if 'mes_ref' in bd_prod.columns:\n",
    "    bd_prod['mes_ref'] = pd.to_datetime(bd_prod['mes_ref'], errors='coerce', dayfirst=True)\n",
    "\n",
    "colunas_manter = ['mes_ref', 'empresa', 'cod_produto', 'descricao', 'linha_prod', 'familia_prod', 'tipo_produto', 'curva_abc', 'curva_123', 'estoq_seg_pcs', 'estoq_seg_kg', 'estoq_inicial_pcs', 'estoq_inicial_kg', 'carteira_arraste_mes_anterior', 'carteira_mes_atual', 'previsao_pcs', 'saldo_previsao_pcs', 'peso_produto_kg', 'estoq_transf_pcs', 'lote_econ', 'qtd_emb', 'lote_min']\n",
    "bd_prod = bd_prod[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Carregar estruturas\n",
    "colunas_bd_estruturas = ['mes_ref', 'empresa', 'cod_prod_acabado', 'cod_insumo', 'descricao', 'qtd_utilizada_pcs']\n",
    "bd_estruturas = pd.read_excel(\n",
    "    pasta_input / 'estruturas.xlsx',\n",
    "    sheet_name='estruturas',\n",
    "    engine='calamine',\n",
    "    dtype={'cod_prod_acabado': str}\n",
    ")\n",
    "bd_estruturas['cod_prod_acabado'] = bd_estruturas['cod_prod_acabado'].astype(str).str.strip().str.upper()\n",
    "bd_estruturas['cod_insumo'] = bd_estruturas['cod_insumo'].astype(str).str.strip().str.upper()\n",
    "bd_estruturas = bd_estruturas[bd_estruturas['cod_prod_acabado'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "bd_estruturas = bd_estruturas[colunas_bd_estruturas]\n",
    "# Elminar coluna mes_ref e remover duplicatas\n",
    "bd_estruturas = bd_estruturas.drop(columns=['mes_ref'])\n",
    "bd_estruturas = bd_estruturas.drop_duplicates(subset=['empresa', 'cod_prod_acabado', 'cod_insumo']).reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Mapeamento de pastas e importação de tabelas concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f84cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Geração de tabelas de calendário concluído!\n"
     ]
    }
   ],
   "source": [
    "# Gerando a tabela Calendário\n",
    "\n",
    "# Definir intervalo de datas: do primeiro mês ao último mês completo\n",
    "data_inicial = bd_prod['mes_ref'].min().replace(day=1)\n",
    "data_final = bd_prod['mes_ref'].max() + MonthEnd(1)\n",
    "\n",
    "# Gerar datas diariamente entre data_inicial e data_final\n",
    "datas = pd.date_range(start=data_inicial, end=data_final, freq='D')\n",
    "\n",
    "# Mapeamento dos dias da semana\n",
    "dias_semana = {0: 'SEG', 1: 'TER', 2: 'QUA', 3: 'QUI', 4: 'SEX', 5: 'SÁB', 6: 'DOM'}\n",
    "\n",
    "# Construção do DataFrame\n",
    "df_calendario = pd.DataFrame({\n",
    "    'data_calend': datas,\n",
    "    'mes_calend': datas.to_series().apply(lambda d: d.replace(day=1)),\n",
    "    'dia_calend': datas.to_series().dt.weekday.map(dias_semana)\n",
    "})\n",
    "\n",
    "# Importar aba Dia_Semana da planilha dados_calendario, com dados por dia da semana x Unidade\n",
    "df_dia_semana = pd.read_excel(pasta_input / 'dados_calendario.xlsx', engine='calamine', sheet_name='Dia_Semana')\n",
    "\n",
    "# Normalização (unpivot)\n",
    "bd_dia_semana = df_dia_semana.melt(\n",
    "    id_vars=['UNIDADE'],\n",
    "    var_name='DIA_DE_SEMANA',\n",
    "    value_name='OCUPACAO'\n",
    ")\n",
    "\n",
    "# Remove linhas onde OCUPACAO está vazia ou inválida (ex: '-')\n",
    "bd_dia_semana = bd_dia_semana[bd_dia_semana['OCUPACAO'].notna()]\n",
    "bd_dia_semana = bd_dia_semana[bd_dia_semana['OCUPACAO'] != '-']\n",
    "bd_dia_semana['OCUPACAO'] = bd_dia_semana['OCUPACAO'].astype(float)\n",
    "\n",
    "# Fazendo o merge entre o calendário e os dados de ocupação por unidade/dia da semana\n",
    "df_calendario_ocup = df_calendario.merge(\n",
    "    bd_dia_semana,\n",
    "    left_on='dia_calend',\n",
    "    right_on='DIA_DE_SEMANA',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Selecionar apenas as colunas desejadas\n",
    "df_calendario_ocup = df_calendario_ocup[\n",
    "    ['data_calend', 'mes_calend', 'dia_calend', 'UNIDADE', 'OCUPACAO']\n",
    "]\n",
    "\n",
    "# Renomear colunas para maiúsculas\n",
    "df_calendario_ocup.columns = [col.upper() for col in df_calendario_ocup.columns]\n",
    "\n",
    "# Importar aba Feriados da planilha dados_calendario, com dados por dia da semana x Unidade\n",
    "df_feriados = pd.read_excel(pasta_input / 'dados_calendario.xlsx', engine='calamine', sheet_name='Feriados')\n",
    "\n",
    "# Garantir que as datas estejam no mesmo formato\n",
    "df_feriados['FERIADO'] = pd.to_datetime(df_feriados['FERIADO'], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Realizar o merge\n",
    "df_calendario_ocup = df_calendario_ocup.merge(\n",
    "    df_feriados[['FERIADO', 'UNIDADE', 'TIPO', 'NORMAL', 'REVEZAMENTO']],\n",
    "    left_on=['DATA_CALEND', 'UNIDADE'],\n",
    "    right_on=['FERIADO', 'UNIDADE'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "data_agora = datetime.now()\n",
    "# Inserindo coluna data e hora agora\n",
    "df_calendario_ocup['DATA_AGORA'] = data_agora\n",
    "\n",
    "# NOVO_NORMAL: Se TIPO for NaN, retorna OCUPACAO; senão, retorna TIPO NORMAL\n",
    "df_calendario_ocup['NOVO_NORMAL'] = df_calendario_ocup.apply(lambda row: row['OCUPACAO'] if pd.isna(row['TIPO']) else row['NORMAL'],axis=1)\n",
    "\n",
    "# NOVO_REVEZAMENTO: Se TIPO for NaN, retorna 1; senão, retorna TIPO REVEZAMENTO\n",
    "df_calendario_ocup['NOVO_REVEZAMENTO'] = df_calendario_ocup.apply(lambda row: 1 if pd.isna(row['TIPO']) else row['REVEZAMENTO'],axis=1)\n",
    "\n",
    "# Coluna DIAS_NOR_CALEND: se DATA_CALEND <= DATA_AGORA retorna 0, senão NOVO_NORMAL\n",
    "df_calendario_ocup['DIAS_NOR_CALEND_'] = df_calendario_ocup.apply(lambda row: 0 if row['DATA_CALEND'] <= row['DATA_AGORA'] else row['NOVO_NORMAL'],axis=1)\n",
    "\n",
    "# Coluna DIAS_REV_CALEND: se DATA_CALEND <= DATA_AGORA retorna 0, senão NOVO_REVEZAMENTO\n",
    "df_calendario_ocup['DIAS_REV_CALEND_'] = df_calendario_ocup.apply(lambda row: 0 if row['DATA_CALEND'] <= row['DATA_AGORA'] else row['NOVO_REVEZAMENTO'],axis=1)\n",
    "\n",
    "# Coluna PARCIAL_HOJE: se DATA_CALEND = DATA_AGORA, RETORNA 1 - (Hora Agora / 24)\n",
    "hora_agora = data_agora.hour + data_agora.minute / 60 + data_agora.second / 3600\n",
    "data_hoje = data_agora.date()\n",
    "\n",
    "df_calendario_ocup['PARCIAL_HOJE'] = df_calendario_ocup['DATA_CALEND'].dt.date.apply(lambda d: 1 - (hora_agora / 24) if d == data_hoje else 0)\n",
    "\n",
    "df_calendario_ocup['DIAS_NOR_CALEND'] = df_calendario_ocup['DIAS_NOR_CALEND_'] + df_calendario_ocup['PARCIAL_HOJE']\n",
    "df_calendario_ocup['DIAS_REV_CALEND'] = df_calendario_ocup['DIAS_REV_CALEND_'] + df_calendario_ocup['PARCIAL_HOJE']\n",
    "\n",
    "bd_calend = df_calendario_ocup.groupby(['MES_CALEND', 'UNIDADE'], as_index=False)[['DIAS_NOR_CALEND', 'DIAS_REV_CALEND']].sum()\n",
    "bd_calend = bd_calend.sort_values(by=['MES_CALEND', 'UNIDADE'])\n",
    "bd_calend = bd_calend.rename(columns={\n",
    "    'DIAS_NOR_CALEND': 'TOT_DIAS_NOR_CALEND',\n",
    "    'DIAS_REV_CALEND': 'TOT_DIAS_REV_CALEND'\n",
    "})\n",
    "\n",
    "bd_calend['TOT_HORAS_NOR_CALEND'] = bd_calend['TOT_DIAS_NOR_CALEND'] * 24\n",
    "bd_calend['TOT_HORAS_REV_CALEND'] = bd_calend['TOT_DIAS_REV_CALEND'] * 24\n",
    "\n",
    "# del bd_dia_semana\n",
    "# del df_calendario_ocup\n",
    "# del df_calendario\n",
    "# del df_feriados\n",
    "# gc.collect()\n",
    "\n",
    "print(\"✅ Geração de tabelas de calendário concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afd7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicio Step = GERAR DADOS LTP\n",
    "bd_prod['NEC_BASE_PROD_PCS'] = (\n",
    "    bd_prod['estoq_inicial_pcs'] \n",
    "    - bd_prod['carteira_mes_atual'] \n",
    "    - bd_prod['carteira_arraste_mes_anterior'] \n",
    "    - bd_prod['previsao_pcs'] \n",
    "    - bd_prod['estoq_seg_pcs']\n",
    ")\n",
    "\n",
    "bd_prod['PRIORIDADE'] = 1\n",
    "\n",
    "bd_mat_param_prioridade_1 = bd_mat_param[bd_mat_param['prioridade'] == 1]\n",
    "\n",
    "# Join completo entre bd_prod e bd_mat_param\n",
    "bd_prod_nec = pd.merge(\n",
    "    bd_mat_param_prioridade_1,\n",
    "    bd_prod,\n",
    "    left_on=['produto', 'unidade_fat', 'prioridade'],\n",
    "    right_on=['cod_produto', 'empresa', 'PRIORIDADE'],\n",
    "    how='outer'  # FULL OUTER JOIN\n",
    ")\n",
    "\n",
    "# Importar aba matriz_regioes da planilha matriz regioes\n",
    "bd_matriz_regioes = pd.read_excel(pasta_input / 'matriz_regioes.xlsx', sheet_name='matriz_regioes', engine='calamine')\n",
    "\n",
    "# Merge para adicionar a Região  de Faturamento na base de produtos\n",
    "bd_prod_nec = bd_prod_nec.merge(\n",
    "    bd_matriz_regioes,\n",
    "    how='left',\n",
    "    left_on='empresa',\n",
    "    right_on='Unidade'\n",
    ")\n",
    "\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Unidade': 'Unidade_Fat'})\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Região': 'Reg_Unid_Fat'})\n",
    "\n",
    "# Merge para adicionar a Região Unidade Produtiva na base de produtos\n",
    "bd_prod_nec = bd_prod_nec.merge(\n",
    "    bd_matriz_regioes,\n",
    "    how='left',\n",
    "    left_on='unidade_prod',\n",
    "    right_on='Unidade'\n",
    ")\n",
    "\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Unidade': 'Unidade_Prod'})\n",
    "bd_prod_nec = bd_prod_nec.rename(columns={'Região': 'Reg_Unid_Prod'})\n",
    "\n",
    "# Criando a coluna MESMA REGIAO, com os critérios aplicados\n",
    "bd_prod_nec['MESMA_REG'] = bd_prod_nec.apply(\n",
    "    lambda row: \"NAO\" if pd.isna(row['Reg_Unid_Prod']) else (\"SIM\" if row['Reg_Unid_Fat'] == row['Reg_Unid_Prod'] else \"NAO\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Inserindo a coluna Mes_Ref_Ant na bd_prod, e criando uma bd_prod_mes_anterior\n",
    "bd_prod_mes_anterior = bd_prod[['mes_ref', 'empresa', 'cod_produto', 'descricao', 'saldo_previsao_pcs']].copy()\n",
    "bd_prod_mes_anterior = bd_prod_mes_anterior.rename(columns=\n",
    "    {'mes_ref': 'MES_REF', \n",
    "     'empresa': 'EMPRESA', \n",
    "     'cod_produto': 'COD_PROD', \n",
    "     'descricao': 'DESC_PROD',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PROX_MES_PCS'\n",
    "     }\n",
    ")\n",
    "\n",
    "# Criando coluna MES_REF_ANT para definir mes do saldo da previsão\n",
    "bd_prod_mes_anterior['MES_REF_ANT'] = (bd_prod_mes_anterior['MES_REF'] - pd.DateOffset(months=1)).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Adicionar a coluna SALDO_PREV_PROX_MES_PCS na bd_prod_nec\n",
    "# Criando um dicionário com as chaves de busca e a coluna que você quer trazer\n",
    "map_dict = bd_prod_mes_anterior.set_index(['MES_REF_ANT', 'EMPRESA', 'COD_PROD'])['SALDO_PREV_PROX_MES_PCS'].to_dict()\n",
    "\n",
    "# Usando map para trazer a coluna 'SALDO_PREV_PROX_MES_PCS' para bd_prod_nec\n",
    "bd_prod_nec['SALDO_PREV_PROX_MES_PCS'] = bd_prod_nec[['mes_ref', 'empresa', 'cod_produto']].apply(\n",
    "    lambda row: map_dict.get((row['mes_ref'], row['empresa'], row['cod_produto']), None), axis=1\n",
    ")\n",
    "\n",
    "# Eliminando linhas filtrando mes_ref que não seja null\n",
    "bd_prod_nec = bd_prod_nec[bd_prod_nec['mes_ref'].notna()]\n",
    "\n",
    "# Criando a coluna PCS_NEC_PROD_MESMA_REG_SIM_NAO\n",
    "# Preencher todos os NaN/None das colunas usadas no cálculo com zero\n",
    "colunas_nec = [\n",
    "    'carteira_arraste_mes_anterior', 'carteira_mes_atual', 'saldo_previsao_pcs',\n",
    "    'SALDO_PREV_PROX_MES_PCS', 'estoq_seg_pcs', 'estoq_inicial_pcs', 'estoq_transf_pcs'\n",
    "]\n",
    "for col in colunas_nec:\n",
    "    bd_prod_nec[col] = pd.to_numeric(bd_prod_nec[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Agora pode aplicar o cálculo normalmente\n",
    "bd_prod_nec['PCS_NEC_PROD_MESMA_REG_SIM_NAO'] = bd_prod_nec.apply(\n",
    "    lambda row: (\n",
    "        row['carteira_arraste_mes_anterior'] + row['carteira_mes_atual'] + row['saldo_previsao_pcs'] + row['SALDO_PREV_PROX_MES_PCS'] + row['estoq_seg_pcs']\n",
    "        - (row['estoq_inicial_pcs'] + row['estoq_transf_pcs'])\n",
    "    ) if row['tipo_produto'] == 'MR'\n",
    "    else (\n",
    "        row['carteira_arraste_mes_anterior'] + row['carteira_mes_atual'] + row['saldo_previsao_pcs'] + row['estoq_seg_pcs']\n",
    "        - (row['estoq_inicial_pcs'] + row['estoq_transf_pcs'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Aplicar substituir null por Zero na coluna SALDO_PREV_PROX_MES_PCS\n",
    "bd_prod_nec['SALDO_PREV_PROX_MES_PCS'] = bd_prod_nec['SALDO_PREV_PROX_MES_PCS'].fillna(0)\n",
    "\n",
    "# Padronizando nomes de colunas para concluir bd_prod_nec\n",
    "bd_prod_nec = bd_prod_nec.rename(columns=\n",
    "    {'mes_ref': 'MES_REF', \n",
    "     'empresa': 'EMPRESA', \n",
    "     'cod_produto': 'COD_PROD', \n",
    "     'descricao': 'DESC_PROD',\n",
    "     'linha_prod': 'LINHA_PROD',\n",
    "     'familia_prod': 'FAMILIA_PROD',\n",
    "     'tipo_produto': 'TIPO_PROD',\n",
    "     'curva_abc': 'CURVA_ABC',\n",
    "     'curva_123': 'CURVA_123',\n",
    "     'estoq_seg_pcs': 'EST_SEG_PCS',\n",
    "     'estoq_inicial_pcs': 'EST_INI_PCS',\n",
    "     'carteira_arraste_mes_anterior': 'CART_ARR_MES_ANT',\n",
    "     'carteira_mes_atual': 'CART_MES_ATUAL',\n",
    "     'previsao_pcs': 'PREV_PCS',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PCS',\n",
    "     'peso_produto_kg': 'PESO_PROD_KG',\n",
    "     'estoq_transf_pcs': 'EST_TRANS_PCS',\n",
    "     'unidade_prod': 'UNID_PROD',\n",
    "     'MESMA_REG': 'MESMA_REG',\n",
    "     'saldo_previsao_pcs': 'SALDO_PREV_PCS',\n",
    "     'lote_econ': 'LOTE_ECON',\n",
    "     'qtd_emb': 'QTD_EMB',\n",
    "     'lote_min': 'LOTE_MIN',\n",
    "     }\n",
    ")\n",
    "\n",
    "# Definindo colunas e encerrando processo de formação da bd_prod_nec\n",
    "bd_prod_nec = bd_prod_nec[\n",
    "    ['MES_REF', 'EMPRESA', 'UNID_PROD', 'MESMA_REG', 'COD_PROD', 'DESC_PROD', \n",
    "     'LINHA_PROD', 'FAMILIA_PROD', 'TIPO_PROD', 'CURVA_ABC','CURVA_123', 'LOTE_ECON','QTD_EMB', 'LOTE_MIN', \n",
    "     'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'PREV_PCS', \n",
    "     'SALDO_PREV_PCS', 'PESO_PROD_KG', 'EST_TRANS_PCS','PCS_NEC_PROD_MESMA_REG_SIM_NAO',\n",
    "     'SALDO_PREV_PROX_MES_PCS']\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f2f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Geração de tabelas de dados LTP concluída!\n"
     ]
    }
   ],
   "source": [
    "### Transformação 04_GerarBaseLTP para gerar a base LTP\n",
    "\n",
    "# Carregar base_dados_roteiros\n",
    "bd_rot = pd.read_excel(\n",
    "    pasta_input / 'base_dados_roteiros.xlsx',\n",
    "    sheet_name='base_dados_roteiros',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "\n",
    "bd_rot['cod_produto'] = bd_rot['cod_produto'].astype(str).str.strip().str.upper()\n",
    "bd_rot = bd_rot[bd_rot['cod_produto'].isin(produtos_param)].copy().reset_index(drop=True)\n",
    "\n",
    "if 'mes_ref' in bd_rot.columns:\n",
    "    bd_rot['mes_ref'] = pd.to_datetime(bd_rot['mes_ref'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "colunas_manter = ['mes_ref', 'empresa', 'cod_produto', 'descricao', 'linha_prod', 'familia_prod', 'tipo_produto', 'cod_ferramenta', 'n_cavidade', 'ciclo_molde', 'pcs_hr', 'mo', 'alocacao_recurso', 'grupo_setor', 'prioridade']\n",
    "\n",
    "bd_rot = bd_rot[colunas_manter].reset_index(drop=True)\n",
    "\n",
    "# Adicionando nova coluna na tabela de roteiros\n",
    "bd_rot['COD_FERR'] = (bd_rot['cod_ferramenta'] + \"|\" + bd_rot['empresa'].str[:3]).str.upper()\n",
    "\n",
    "# Faz o merge trazendo somente as 4 colunas desejadas, associando pelos campos de chave\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa'], \n",
    "    right_on=['MES_CALEND', 'UNIDADE']\n",
    ")\n",
    "\n",
    "# Removendo as colunas que não quero na bd_rot\n",
    "colunas_excluir = ['MES_CALEND', 'UNIDADE']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Carregando o Calendario de Recursos\n",
    "bd_calend_rec = pd.read_excel(pasta_input / 'calend_recursos.xlsx', sheet_name='calend_recursos')\n",
    "\n",
    "# Inserindo coluna PER_TURNO no calendário de recursos\n",
    "bd_calend_rec['PER_TURNO'] = bd_calend_rec['turno_maq'] / 3\n",
    "\n",
    "# Inserindo colunas do calendário de recursos na bd_rot\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend_rec, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa', 'alocacao_recurso'], \n",
    "    right_on=['mes_ref', 'unidade', 'maq']\n",
    ")\n",
    "\n",
    "colunas_excluir = ['unidade', 'maq', 'setores', 'turno_maq']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "bd_rot.rename(columns={\n",
    "    'revezamento': 'REC_revezamento',\n",
    "    'horas_extras': 'REC_horas_extras',\n",
    "    'disp_mes': 'REC_disp_mes',\n",
    "    'horas_indisp': 'REC_horas_indisp',\n",
    "    'PER_TURNO': 'REC_PER_TURNO'\n",
    "}, inplace=True)\n",
    "\n",
    "# Carregando Calendário de Ferramentas\n",
    "bd_calend_ferr = pd.read_excel(pasta_input / 'calend_ferramentas.xlsx', sheet_name='calend_ferramentas')\n",
    "bd_calend_ferr['NEW_MOLDES'] = bd_calend_ferr['moldes'].str.upper() + \"|\" + bd_calend_ferr['unidade'].str.upper()\n",
    "\n",
    "# Merge entre bd_rot e bd_calend_ferr\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_calend_ferr, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'empresa', 'cod_ferramenta'], \n",
    "    right_on=['mes_ref', 'unidade', 'moldes']\n",
    ")\n",
    "\n",
    "colunas_excluir = ['moldes','unidade', 'setores']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "bd_rot.rename(columns={\n",
    "    'horas_indisp': 'FER_horas_indisp',\n",
    "    'disp_mes': 'FER_disp_mes',\n",
    "    'NEW_MOLDES': 'moldes'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preencher apenas os valores nulos de FER_horas_indisp com zero\n",
    "bd_rot['FER_horas_indisp'] = bd_rot['FER_horas_indisp'].fillna(0)\n",
    "\n",
    "# Calculando coluna HOR_REC_C1\n",
    "# Se REC_revezamento for \"SIM\", calcula com TOT_HORAS_REV_CALEND, senão com TOT_HORAS_NOR_CALEND\n",
    "# Calculando coluna HOR_REC_C1, se algum valor for nulo, retorna 0\n",
    "bd_rot['HOR_REC_C1'] = np.where(\n",
    "    bd_rot[['TOT_HORAS_REV_CALEND', 'REC_horas_extras', 'REC_horas_indisp', 'REC_PER_TURNO', 'REC_disp_mes', 'TOT_HORAS_NOR_CALEND']].isnull().any(axis=1),\n",
    "    0,\n",
    "    np.where(\n",
    "        bd_rot['REC_revezamento'] == \"SIM\",\n",
    "        ((bd_rot['TOT_HORAS_REV_CALEND'] + bd_rot['REC_horas_extras'] - bd_rot['REC_horas_indisp']) * bd_rot['REC_PER_TURNO']) * bd_rot['REC_disp_mes'],\n",
    "        ((bd_rot['TOT_HORAS_NOR_CALEND'] + bd_rot['REC_horas_extras'] - bd_rot['REC_horas_indisp']) * bd_rot['REC_PER_TURNO']) * bd_rot['REC_disp_mes']\n",
    "    )\n",
    ").round(2)\n",
    "\n",
    "# Calculando coluna HOR_FERR_C1\n",
    "# Se REC_revezamento for \"SIM\", calcula com TOT_HORAS_REV_CALEND, senão com TOT_HORAS_NOR_CALEND\n",
    "# Calculando coluna HOR_FER_C1, se algum valor for nulo, retorna 0, senão calcula conforme regra, com duas casas decimais\n",
    "bd_rot['HOR_FER_C1'] = np.where(\n",
    "    bd_rot[['TOT_HORAS_REV_CALEND', 'REC_horas_extras', 'FER_horas_indisp', 'TOT_HORAS_NOR_CALEND', 'FER_disp_mes']].isnull().any(axis=1),\n",
    "    0,\n",
    "    np.where(\n",
    "        bd_rot['REC_revezamento'] == \"SIM\",\n",
    "        (bd_rot['TOT_HORAS_REV_CALEND'] + (bd_rot['REC_horas_extras'] - bd_rot['FER_horas_indisp'])) * bd_rot['FER_disp_mes'],\n",
    "        (bd_rot['TOT_HORAS_NOR_CALEND'] + (bd_rot['REC_horas_extras'] - bd_rot['FER_horas_indisp'])) * bd_rot['FER_disp_mes']\n",
    "    )\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_REC conforme a regra\n",
    "# Se HOR_REC_C1 for null ou menor que 0, atribui 0; senão, atribui o valor de HOR_REC_C1\n",
    "bd_rot['HOR_REC'] = np.where(\n",
    "    bd_rot['HOR_REC_C1'].isnull() | (bd_rot['HOR_REC_C1'] < 0),\n",
    "    0,\n",
    "    bd_rot['HOR_REC_C1']\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_FER conforme a regra\n",
    "# Se HOR_FER_C1 for null ou menor que 0, atribui 0; senão, atribui o valor de HOR_FER_C1\n",
    "bd_rot['HOR_FER'] = np.where(\n",
    "    bd_rot['HOR_FER_C1'].isnull() | (bd_rot['HOR_FER_C1'] < 0),\n",
    "    0,\n",
    "    bd_rot['HOR_FER_C1']\n",
    ").round(2)\n",
    "\n",
    "# Criando coluna HOR_CAP conforme a regra\n",
    "# Se HOR_REC for menor que HOR_FER, atribui HOR_REC; senão, atribui HOR_FER\n",
    "bd_rot['HOR_CAP'] = np.where(\n",
    "    bd_rot['HOR_REC'] < bd_rot['HOR_FER'],\n",
    "    bd_rot['HOR_REC'],\n",
    "    bd_rot['HOR_FER']\n",
    ").round(2)\n",
    "\n",
    "# Eliminando colunas para tornar mais clean a bd_rot\n",
    "colunas_excluir = [\n",
    "    'REC_revezamento', 'REC_horas_extras', 'REC_disp_mes', 'REC_horas_indisp',\n",
    "    'TOT_HORAS_REV_CALEND', 'TOT_HORAS_NOR_CALEND', 'REC_PER_TURNO',\n",
    "    'FER_horas_indisp', 'HOR_REC_C1', 'HOR_FER_C1', 'disp_mes'\n",
    "]\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando a coluna prioridade do bd_rot para PRIORIDADE_ROTEIRO, para evitar confusão \n",
    "# com a prioridade da bd_mat_param\n",
    "bd_rot.rename(columns={'prioridade': 'PRIORIDADE_ROTEIRO'}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_mat_param para trazer os campos unidade_fat e prioridade\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_mat_param, \n",
    "    how='left', \n",
    "    left_on=['empresa', 'cod_produto'], \n",
    "    right_on=['unidade_prod', 'produto']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = ['produto', 'unidade_prod', 'tipo_abast']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando colunas da matriz parametros que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'unidade_fat': 'MAT_PAR_unidade_fat',\n",
    "    'prioridade': 'MAT_PAR_prioridade'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_prod_necs para trazer os campos de volumese e dados\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_prod_nec, \n",
    "    how='left', \n",
    "    left_on=['mes_ref', 'MAT_PAR_unidade_fat', 'cod_produto'], \n",
    "    right_on=['MES_REF', 'EMPRESA', 'COD_PROD']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = [\n",
    "    'TOT_DIAS_NOR_CALEND','TOT_DIAS_REV_CALEND', 'moldes', 'MES_REF', 'EMPRESA',\n",
    "    'UNID_PROD', 'COD_PROD', 'DESC_PROD', 'LINHA_PROD',\n",
    "    'FAMILIA_PROD', 'TIPO_PROD', 'CURVA_ABC', 'CURVA_123'\n",
    "]\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando da bd_roteiro que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'mes_ref': 'MES_REF',\n",
    "    'empresa': 'EMPRESA',\n",
    "    'cod_produto': 'COD_PROD',\n",
    "    'descricao': 'DESC_PROD',\n",
    "    'linha_prod': 'LINHA_PROD',\n",
    "    'familia_prod': 'FAMILIA_PROD',\n",
    "    'tipo_produto': 'TIPO_PROD',\n",
    "    'COD_FERR': 'COD_FER_UNID',\n",
    "    'cod_ferramenta': 'COD_FERR',\n",
    "    'n_cavidade': 'N_CAVIDADE',\n",
    "    'ciclo_molde': 'CICLO_MOLDE',\n",
    "    'pcs_hr': 'PCS_HORA',\n",
    "    'mo': 'MO',\n",
    "    'alocacao_recurso': 'ALOC_REC',\n",
    "    'grupo_setor': 'GRUPO_SETOR',\n",
    "    'peso_produto_kg': 'PESO_PROD',\n",
    "    'PRIORIDADE_ROTEIRO': 'PRIOR_ROT',\n",
    "    'MAT_PAR_unidade_fat': 'UNID_FAT_MATPAR',\n",
    "    'MAT_PAR_prioridade': 'PRIOR_MATPAR'\n",
    "}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_regioes para trazer os campo região fat\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_regioes, \n",
    "    how='left', \n",
    "    left_on=['UNID_FAT_MATPAR'], \n",
    "    right_on=['Unidade']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "bd_rot = drop_colunas(bd_rot, ['Unidade'])\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'Região': 'REG_UNID_FAT'}, inplace=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_regioes para trazer os campo região prod\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_regioes, \n",
    "    how='left', \n",
    "    left_on=['EMPRESA'], \n",
    "    right_on=['Unidade']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "bd_rot = drop_colunas(bd_rot, ['Unidade'])\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'Região': 'REG_UNID_PROD'}, inplace=True)\n",
    "\n",
    "# Criando o campo MESMA_REG conforme a regra solicitada\n",
    "bd_rot['MESMA_REG'] = np.where(\n",
    "    bd_rot['REG_UNID_PROD'].isna(),\n",
    "    \"NAO\",\n",
    "    np.where(\n",
    "        bd_rot['REG_UNID_FAT'] == bd_rot['REG_UNID_PROD'],\n",
    "        \"SIM\",\n",
    "        \"NAO\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Substituir null (NaN) por 0 nos campos solicitados\n",
    "campos_zerar = [\n",
    "    'EST_SEG_PCS', 'EST_INI_PCS',\n",
    "    'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'PREV_PCS', 'SALDO_PREV_PCS',\n",
    "    'PESO_PROD_KG', 'EST_TRANS_PCS', 'SALDO_PREV_PROX_MES_PCS', 'PCS_NEC_PROD_MESMA_REG_SIM_NAO'\n",
    "]\n",
    "bd_rot[campos_zerar] = bd_rot[campos_zerar].fillna(0)\n",
    "\n",
    "# Criando a coluna MES_REF_ANT reduzindo 1 mês de MES_REF\n",
    "bd_rot['MES_REF_ANT'] = (bd_rot['MES_REF'] - pd.DateOffset(months=1)).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Renomeando da bd_roteiro que foram trazidas para a bd_rot\n",
    "bd_rot.rename(columns={\n",
    "    'EMPRESA': 'UNID_PROD',\n",
    "    'UNID_FAT_MATPAR': 'UNID_FAT',\n",
    "}, inplace=True)\n",
    "\n",
    "# Importar aba matriz_regioes da planilha matriz limitantes\n",
    "bd_matriz_limitantes = pd.read_excel(pasta_input / 'matriz_limitantes.xlsx', sheet_name='matriz_limitantes', dtype={'produto': str})\n",
    "\n",
    "if 'mes_ref' in bd_matriz_limitantes.columns:\n",
    "    bd_matriz_limitantes['mes_ref'] = pd.to_datetime(bd_matriz_limitantes['mes_ref'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Executando o merge entre bd_rot e bd_matriz_limitantes para trazer os campo limitante_pcs\n",
    "bd_rot = bd_rot.merge(\n",
    "    bd_matriz_limitantes, \n",
    "    how='left',\n",
    "    left_on=['MES_REF','UNID_FAT','COD_PROD'],\n",
    "    right_on=['mes_ref','unidade','produto']\n",
    ")\n",
    "\n",
    "# Elimnando campos que não são mais necessários da bd_rot\n",
    "colunas_excluir = ['mes_ref','unidade','produto']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Renomeando da campos que foram trazidos para a bd_rot\n",
    "bd_rot.rename(columns={'limitante_pcs': 'LIMIT_PCS'}, inplace=True)\n",
    "\n",
    "# Aplicando a regra no campo LIMIT_PCS conforme lógica Java\n",
    "bd_rot['AVALIAR'] = bd_rot['PRIOR_MATPAR'].astype(str) + \"|\" + bd_rot['PRIOR_ROT'].astype(str)\n",
    "bd_rot['LIMIT_PCS'] = np.where(\n",
    "    (bd_rot['AVALIAR'] != \"1|1\") | (bd_rot['LIMIT_PCS'].isnull()),\n",
    "    0,\n",
    "    bd_rot['LIMIT_PCS']\n",
    ")\n",
    "\n",
    "# Formatando a coluna PRIOR_MATPAR para inteiro\n",
    "bd_rot['PRIOR_MATPAR'] = bd_rot['PRIOR_MATPAR'].fillna(0).astype(int)\n",
    "\n",
    "# Eliminar a coluna AVALIAR\n",
    "colunas_excluir = ['AVALIAR', 'MES_REF_ANT']\n",
    "bd_rot = drop_colunas(bd_rot, colunas_excluir)\n",
    "\n",
    "# Criar ID_COMP\n",
    "bd_rot['ID_COMP'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_PROD'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_RECURSO\n",
    "bd_rot['ID_RECURSO'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['ALOC_REC'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_FERRAMENTA\n",
    "bd_rot['ID_FERRAMENTA'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_PROD'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_FER_UNID'].str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_PROD_UNID_FAT\n",
    "bd_rot['ID_PROD_UNID_FAT'] = (\n",
    "    bd_rot['MES_REF'].dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_FAT'].str.upper() + \"|\" +\n",
    "    bd_rot['COD_PROD'].astype(str).str.upper()\n",
    ")\n",
    "\n",
    "# Criar ID_PROD_UNID_FAT_ANT já calculando o mês anterior no mesmo comando\n",
    "bd_rot['ID_PROD_UNID_FAT_ANT'] = (\n",
    "    (bd_rot['MES_REF'] - pd.DateOffset(months=1)).dt.strftime('%b%y').str.upper() + \"|\" +\n",
    "    bd_rot['UNID_FAT'].str.upper()  + \"|\" +\n",
    "    bd_rot['COD_PROD'].str.upper()\n",
    ")\n",
    " \n",
    "# Criar a coluna IND\n",
    "# Classificação antiga\n",
    "# bd_rot = bd_rot.sort_values(by=['MES_REF', 'UNID_FAT', 'COD_PROD', 'PRIOR_MATPAR', 'PRIOR_ROT', 'TIPO_PROD']).reset_index(drop=True)\n",
    "\n",
    "# Classificação nova\n",
    "bd_rot = bd_rot.sort_values(by=['MES_REF', 'TIPO_PROD', 'UNID_FAT', 'COD_PROD', 'PRIOR_MATPAR', 'PRIOR_ROT']).reset_index(drop=True)\n",
    "bd_rot['IND'] = range(1, len(bd_rot) + 1)\n",
    "\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_REC ||||||||||||||||||||||||||||||||\n",
    "# Remover linhas onde ID_RECURSO é NaN\n",
    "bd_rot = bd_rot[bd_rot['ID_RECURSO'].notna()].copy()\n",
    "\n",
    "# Ordena primeiro por ID_RECURSO e depois por IND\n",
    "# bd_rot = bd_rot.sort_values(by=['ID_RECURSO', 'IND']).reset_index(drop=True)\n",
    "\n",
    "# Cria o índice incremental por ID_RECURSO, começando em 1\n",
    "# bd_rot['ID_NUM_REC'] = (bd_rot.groupby('ID_RECURSO').cumcount() + 1).astype(int)\n",
    "\n",
    "# Classificar crescente pela coluna IND\n",
    "bd_rot = bd_rot.sort_values(by=['IND']).reset_index(drop=True)\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_REC ||||||||||||||||||||||||||||||||\n",
    "\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_FER ||||||||||||||||||||||||||||||||\n",
    "# Remover linhas onde ID_FERRAMENTA é NaN\n",
    "bd_rot = bd_rot[bd_rot['ID_FERRAMENTA'].notna()].copy()\n",
    "\n",
    "# # Ordena primeiro por ID_NUM_FER e depois por IND\n",
    "# bd_rot = bd_rot.sort_values(by=['ID_FERRAMENTA', 'IND']).reset_index(drop=True)\n",
    "\n",
    "# # Cria o índice incremental por ID_NUM_FER, começando em 1\n",
    "# bd_rot['ID_NUM_FER'] = (bd_rot.groupby('ID_FERRAMENTA').cumcount() + 1).astype(int)\n",
    "\n",
    "# Classificar crescente pela coluna IND\n",
    "bd_rot = bd_rot.sort_values(by=['IND']).reset_index(drop=True)\n",
    "# |||||||||||||||||||||||||||||||| ID_NUM_FER ||||||||||||||||||||||||||||||||\n",
    "\n",
    "# Criar coluna ID_CONC_REC = ID_RECURSO + \"|\" + ID_NUM_REC e ID_CONC_FER = ID_FERRAMENTA + \"|\" + ID_NUM_FER\n",
    "# bd_rot['ID_CONC_REC'] = bd_rot['ID_RECURSO'] + \"|\" + bd_rot['ID_NUM_REC'].astype(str)\n",
    "# bd_rot['ID_CONC_FER'] = bd_rot['ID_FERRAMENTA'] + \"|\" + bd_rot['ID_NUM_FER'].astype(str)\n",
    "\n",
    "# Organizando Layout Final da para dados para bd_base_LTP\n",
    "# Ordenando colunas\n",
    "nova_ordem_colunas = [\n",
    "    'MES_REF', 'UNID_FAT', 'UNID_PROD', 'MESMA_REG', 'PRIOR_MATPAR', 'PRIOR_ROT', 'COD_PROD', 'DESC_PROD', 'LINHA_PROD',\n",
    "    'FAMILIA_PROD', 'TIPO_PROD', 'COD_FER_UNID', 'N_CAVIDADE', 'CICLO_MOLDE', 'PCS_HORA', 'MO', 'ALOC_REC', \n",
    "    'GRUPO_SETOR', 'PESO_PROD_KG', 'LOTE_ECON', 'QTD_EMB','LOTE_MIN', 'IND', 'ID_COMP', 'ID_RECURSO', 'ID_FERRAMENTA', 'ID_PROD_UNID_FAT', 'ID_PROD_UNID_FAT_ANT', 'HOR_REC', 'HOR_FER', 'HOR_CAP', 'PREV_PCS', 'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'SALDO_PREV_PCS', 'EST_TRANS_PCS', 'SALDO_PREV_PROX_MES_PCS', 'LIMIT_PCS'\n",
    "    ]\n",
    "\n",
    "# # Reordenando as colunas\n",
    "bd_prod_rot = bd_rot[nova_ordem_colunas].reset_index(drop=True)\n",
    "\n",
    "# Criar tabela com ID_PROD_UNID_FAT e MESMA_REG, filtrando PRIOR_MATPAR = 1 e PRIOR_ROT = 1\n",
    "bd_prod_rot_PRIOR_11 = bd_prod_rot[\n",
    "    (bd_prod_rot['PRIOR_MATPAR'] == 1) & (bd_prod_rot['PRIOR_ROT'] == 1)\n",
    "][['ID_PROD_UNID_FAT', 'MESMA_REG']].drop_duplicates().reset_index(drop=True)\n",
    "bd_prod_rot_PRIOR_11.rename(columns={'MESMA_REG': 'MESMA_REG_PRIOR_11'}, inplace=True)\n",
    "\n",
    "# Fazer join entre bd_prod_rot e bd_prod_rot_PRIOR_11 para trazer MESMA_REG_PRIOR_11\n",
    "bd_prod_rot = bd_prod_rot.merge(\n",
    "    bd_prod_rot_PRIOR_11,\n",
    "    how='left',\n",
    "    on='ID_PROD_UNID_FAT'\n",
    ")\n",
    "\n",
    "# Substituir coluna MESMA_REG por MESMA_REG_PRIOR_11\n",
    "bd_prod_rot['MESMA_REG'] = bd_prod_rot['MESMA_REG_PRIOR_11'].fillna(bd_prod_rot['MESMA_REG'])\n",
    "# Eliminar coluna MESMA_REG_PRIOR_11\n",
    "bd_prod_rot = drop_colunas(bd_prod_rot, ['MESMA_REG_PRIOR_11'])\n",
    "\n",
    "# Criar bd_estrutura_filtrada, com base na bd_estruturas eliminando cod_insumo que não constam na coluna COD_PROD da bd_prod_rot\n",
    "# codigos_validos = set(bd_prod_rot['COD_PROD'].unique())\n",
    "# bd_estrutura_filtrada = bd_estruturas[bd_estruturas['cod_insumo'].isin(codigos_validos)].copy().reset_index(drop=True)\n",
    "\n",
    "# Criar conjunto de tuplas válidas (COD_PROD, UNID_PROD)\n",
    "codigos_validos = set(zip(bd_prod_rot['COD_PROD'], bd_prod_rot['UNID_PROD']))\n",
    "\n",
    "# Filtrar estrutura com base em cod_insumo + empresa\n",
    "bd_estrutura_filtrada = (\n",
    "    bd_estruturas[\n",
    "        bd_estruturas[['cod_insumo', 'empresa']].apply(tuple, axis=1).isin(codigos_validos)\n",
    "    ]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Mesclar bd_base_LTP e bd_estrutura_filtrada por UNID_PROD, COD_PROD vs empresa e cod_prod_acabado\n",
    "bd_prod_rot_estr = pd.merge(bd_prod_rot, bd_estrutura_filtrada, how='left', left_on=['UNID_PROD', 'COD_PROD'], right_on=['empresa', 'cod_prod_acabado']).reset_index(drop=True)\n",
    "\n",
    "# Função para criar estrutura com fator estrutural\n",
    "bd_estr_fator_estrutural = criar_estrutura_com_fator_estrutural(bd_estrutura_filtrada)\n",
    "\n",
    "print(\"✅ Geração de tabelas de dados LTP concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e00fe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Primeiro Calculo NEC_PCS e Distribuição Capacidade concluídos!\n"
     ]
    }
   ],
   "source": [
    "########### TRAZER CAMPOS DA MATRIZ TRANSFERENCIA E ESTOQUE ORIGEM ###########\n",
    "bd_LTP_NEC = bd_prod_rot.copy()\n",
    "\n",
    "# Trazer para bd_produtos_nec as coluna de status da tabela bd_mat_transf_comp, colunas considera_estoq_origem e considera_estoq_triangulacao\n",
    "\n",
    " # Carregar matriz_transf_componentes\n",
    "bd_mat_transf_comp = pd.read_excel(\n",
    "    pasta_input / 'matriz_transf_componentes.xlsx',\n",
    "    sheet_name='matriz_transf_componentes',\n",
    "    dtype={'cod_produto': str}\n",
    ")\n",
    "bd_mat_transf_comp['cod_produto'] = bd_mat_transf_comp['cod_produto'].astype(str).str.strip().str.upper()\n",
    "# Filtrar o DataFrame bd_mat_transf_comp para manter apenas as linhas onde 'validacao' == 'Sim'\n",
    "bd_mat_transf_comp = bd_mat_transf_comp[bd_mat_transf_comp['validacao'].str.upper() == 'SIM']\n",
    "bd_mat_transf_comp = bd_mat_transf_comp.reset_index(drop=True)\n",
    "\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_mat_transf_comp[['unid_destino', 'unid_origem', 'unid_triangulacao', 'cod_produto', 'considera_estoq_origem', 'considera_estoq_triangulacao']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_destino', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Trazer os valores de Estoque Inicial e Estoque Transferencia das unidades Origem\n",
    "# Cria uma cópia da tabela para servir como origem\n",
    "bd_produtos_origem = bd_LTP_NEC[['UNID_FAT', 'MES_REF', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS']].copy()\n",
    "bd_produtos_origem = bd_produtos_origem.rename(columns={\n",
    "    'UNID_FAT': 'UNID_FAT_ORIGEM',\n",
    "    'MES_REF': 'MES_REF_ORIGEM',\n",
    "    'COD_PROD': 'COD_PROD_ORIGEM',\n",
    "    'EST_INI_PCS': 'EST_INI_PCS_ORIGEM',\n",
    "    'EST_TRANS_PCS': 'EST_TRANS_PCS_ORIGEM'\n",
    "})\n",
    "\n",
    "# Eliminar duplicatas da tabela de origem\n",
    "bd_produtos_origem = bd_produtos_origem.drop_duplicates(subset=['UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM', 'EST_INI_PCS_ORIGEM', 'EST_TRANS_PCS_ORIGEM'])\n",
    "\n",
    "# Faz o merge na própria tabela, buscando os valores da origem\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_produtos_origem,\n",
    "    how='left',\n",
    "    left_on=['unid_origem', 'MES_REF', 'COD_PROD'],\n",
    "    right_on=['UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM']\n",
    ")\n",
    "\n",
    "# Criar uma tabela com cópia na bd_produtos_nec ter somente as colunas de interesse e buscar os valores de estoque origem e estoque triangulação\n",
    "bd_produtos_estoque_origem_triangulacao = bd_LTP_NEC\n",
    "colunas_excluir = ['considera_estoq_origem', 'considera_estoq_triangulacao']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "# Trazer para bd_produtos_nec as colunas de status da tabela bd_mat_transf_comp, colunas considera_estoq_origem e considera_estoq_triangulacao\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao.merge(\n",
    "    bd_mat_transf_comp[['unid_origem', 'cod_produto', 'considera_estoq_origem']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_origem', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Excluir as colunas unid_triangulacao e cod_produto\n",
    "colunas_excluir = ['unid_triangulacao', 'cod_produto']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao.merge(\n",
    "    bd_mat_transf_comp[['unid_triangulacao', 'cod_produto', 'considera_estoq_triangulacao']],\n",
    "    how='left',\n",
    "    left_on=['UNID_FAT', 'COD_PROD'],\n",
    "    right_on=['unid_triangulacao', 'cod_produto']\n",
    ")\n",
    "\n",
    "# Excluir as colunas unid_destino e cod_produto\n",
    "colunas_excluir = ['unid_triangulacao', 'cod_produto']\n",
    "bd_produtos_estoque_origem_triangulacao = drop_colunas(bd_produtos_estoque_origem_triangulacao, colunas_excluir)\n",
    "\n",
    "bd_produtos_estoque_origem_triangulacao = bd_produtos_estoque_origem_triangulacao[['MES_REF', 'UNID_FAT', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS','considera_estoq_origem', 'considera_estoq_triangulacao']]\n",
    "\n",
    "bd_produtos_estoque_origem_triangulacao.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Trazer os valores de Estoque Inicial e Estoque Transferencia das unidades Triangulação\n",
    "# Cria uma cópia da tabela para servir como Triangulação\n",
    "bd_produtos_triangulacao = bd_LTP_NEC[['UNID_FAT', 'MES_REF', 'COD_PROD', 'EST_INI_PCS', 'EST_TRANS_PCS']].copy()\n",
    "\n",
    "bd_produtos_triangulacao = bd_produtos_triangulacao.rename(columns={\n",
    "    'UNID_FAT': 'UNID_FAT_TRIANG',\n",
    "    'MES_REF': 'MES_REF_TRIANG',\n",
    "    'COD_PROD': 'COD_PROD_TRIANG',\n",
    "    'EST_INI_PCS': 'EST_INI_PCS_TRIANG',\n",
    "    'EST_TRANS_PCS': 'EST_TRANS_PCS_TRIANG'\n",
    "})\n",
    "\n",
    "# Faz o merge na própria tabela, buscando os valores da TRIANG\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_produtos_triangulacao,\n",
    "    how='left',\n",
    "    left_on=['unid_triangulacao', 'MES_REF', 'COD_PROD'],\n",
    "    right_on=['UNID_FAT_TRIANG', 'MES_REF_TRIANG', 'COD_PROD_TRIANG']\n",
    ")\n",
    "\n",
    "# Somar valores Estoque Inicial e Transferencia da Origem\n",
    "def ORI_TOT_PCS(row):\n",
    "    if str(row.get('considera_estoq_origem', '')).strip().upper() == 'SIM':\n",
    "        return row.get('EST_INI_PCS_ORIGEM', 0) + row.get('EST_TRANS_PCS_ORIGEM', 0)\n",
    "    else:\n",
    "        return 0\n",
    "bd_LTP_NEC['ORI_TOT_PCS'] = bd_LTP_NEC.apply(ORI_TOT_PCS, axis=1)\n",
    "\n",
    "# Somar valores Estoque Inicial e Transferencia da Triangulação\n",
    "def TRIANG_TOT_PCS(row):\n",
    "    if str(row.get('considera_estoq_triangulacao', '')).strip().upper() == 'SIM':\n",
    "        return row.get('EST_INI_PCS_TRIANG', 0) + row.get('EST_TRANS_PCS_TRIANG', 0)\n",
    "    else:\n",
    "        return 0\n",
    "bd_LTP_NEC['TRIANG_TOT_PCS'] = bd_LTP_NEC.apply(TRIANG_TOT_PCS, axis=1)\n",
    "\n",
    "# Eliminar colunas desnecessárias da bd_LTP_NEC\n",
    "colunas_excluir = [\n",
    "    'unid_destino', 'unid_origem', 'unid_triangulacao', 'cod_produto',\n",
    "    'considera_estoq_origem', 'considera_estoq_triangulacao',\n",
    "    'UNID_FAT_ORIGEM', 'MES_REF_ORIGEM', 'COD_PROD_ORIGEM',\n",
    "    'EST_INI_PCS_ORIGEM', 'EST_TRANS_PCS_ORIGEM',\n",
    "    'UNID_FAT_TRIANG', 'MES_REF_TRIANG', 'COD_PROD_TRIANG',\n",
    "    'EST_INI_PCS_TRIANG', 'EST_TRANS_PCS_TRIANG'\n",
    "]\n",
    "bd_LTP_NEC = drop_colunas(bd_LTP_NEC, colunas_excluir)\n",
    "\n",
    "# Aplicar zero nos campos vazios de NEC_PCS, ORI_TOT_PCS e TRIANG_TOT_PCS\n",
    "colunas_preencher = ['ORI_TOT_PCS', 'TRIANG_TOT_PCS']\n",
    "bd_LTP_NEC[colunas_preencher] = bd_LTP_NEC[colunas_preencher].fillna(0).astype(float)\n",
    "\n",
    "# Filtrar bd_LTP_NEC pelo campo MAT_PAR_prioridade > 0\n",
    "bd_LTP_NEC = bd_LTP_NEC[bd_LTP_NEC['PRIOR_MATPAR'] > 0].reset_index(drop=True)\n",
    "\n",
    "#*****************************************# ID_ULT_PRIORI #*****************************************\n",
    "# Criar a tabela bd_Ultimo_Roteiro_MAT_PAR com os campos ID_PROD_UNID_FAT, PRIOR_MATPAR e PRIOR_ROT, eliminando as duplicatas\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_LTP_NEC[['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT']]\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_Ultimo_Roteiro_MAT_PAR.sort_values(\n",
    "    by=['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT'],\n",
    "    ascending=[True, True, False]\n",
    ").reset_index(drop=True)\n",
    "bd_Ultimo_Roteiro_MAT_PAR = bd_Ultimo_Roteiro_MAT_PAR.drop_duplicates(subset='ID_PROD_UNID_FAT', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Criar a coluna ID_ULT_PRIORI fazendo o merge entre bd_LTP_NEC e bd_Ultimo_Roteiro_MAT_PAR para trazer dados a coluna ID_PROD_UNID_FAT\n",
    "bd_LTP_NEC = bd_LTP_NEC.merge(\n",
    "    bd_Ultimo_Roteiro_MAT_PAR[['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT']].rename(columns={'ID_PROD_UNID_FAT': 'ID_ULT_PRIORI'}),\n",
    "    how='left',\n",
    "    left_on=['ID_PROD_UNID_FAT', 'PRIOR_MATPAR', 'PRIOR_ROT'],\n",
    "    right_on=['ID_ULT_PRIORI', 'PRIOR_MATPAR', 'PRIOR_ROT']\n",
    ")\n",
    "\n",
    "bd_LTP_NEC = bd_LTP_NEC.sort_values(['IND']).reset_index(drop=True)\n",
    "\n",
    "# Zerar valores campo SALDO_PREV_PROX_MES_PCS, considerando se MESMA_REG for igual a \"SIM\", manter valor, se MESMA_REG for \"NAO\", zerar o valor\n",
    "bd_LTP_NEC['SALDO_PREV_PROX_MES_PCS'] = np.where(\n",
    "    bd_LTP_NEC['MESMA_REG'] == 'NAO',\n",
    "    bd_LTP_NEC['SALDO_PREV_PROX_MES_PCS'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Criando cópia dos campos e adicinar nos rótulos, no incio do nome LTP\n",
    "campos_copiar = [\n",
    "    'EST_SEG_PCS', 'EST_INI_PCS', 'CART_ARR_MES_ANT', 'CART_MES_ATUAL', 'SALDO_PREV_PCS', 'EST_TRANS_PCS','SALDO_PREV_PROX_MES_PCS'\n",
    "]\n",
    "\n",
    "for col in campos_copiar:\n",
    "    bd_LTP_NEC['LTP_' + col] = bd_LTP_NEC[col]\n",
    "    \n",
    "# Adicionar coluna LTP_COMP_NEC_PCS com valor 0\n",
    "bd_LTP_NEC['LTP_COMP_NEC_PCS'] = 0\n",
    "\n",
    "# Utilizando Flags Parametros Lote Minimo e Multiplo Embalagens e Calculando os campos VAR_NEC1, VAR_NEC2 e VAR_NEC3\n",
    "def carregar_flags_ltp(pasta_painel):\n",
    "    def carregar_planilha_segura(caminho: Path, **kwargs):\n",
    "        try:\n",
    "            return load_workbook(caminho, **kwargs)\n",
    "        except PermissionError:\n",
    "            with tempfile.NamedTemporaryFile(suffix=caminho.suffix, delete=False) as tmp:\n",
    "                shutil.copy2(caminho, tmp.name)\n",
    "                return load_workbook(tmp.name, **kwargs)\n",
    "\n",
    "    def obter_valor_nome_definido(wb, nome_definido):\n",
    "        nome_planilha, intervalo = next(nome_definido.destinations)\n",
    "        planilha = wb[nome_planilha]\n",
    "        coluna_min, linha_min, _, _ = range_boundaries(intervalo)\n",
    "        return planilha.cell(linha_min, coluna_min).value\n",
    "\n",
    "    arquivo_painel = pasta_painel / 'Painel_LTP.xlsm'\n",
    "    planilha_ltp = carregar_planilha_segura(arquivo_painel, data_only=True)\n",
    "    nome_lote_min = planilha_ltp.defined_names['FlagLoteMinimo']\n",
    "    nome_multiplo_emb = planilha_ltp.defined_names['FlagMultiploEmb']\n",
    "    lote_min = obter_valor_nome_definido(planilha_ltp, nome_lote_min)\n",
    "    multiplo_emb = obter_valor_nome_definido(planilha_ltp, nome_multiplo_emb)\n",
    "    return str(lote_min).strip().upper(), str(multiplo_emb).strip().upper()\n",
    "\n",
    "lote_min_flag, multiplo_emb_flag = carregar_flags_ltp(pasta_painel)\n",
    "\n",
    "# Mover Colunas ID_RECURSO e id_FERRAMENTA para o final do DataFrame, depois da coluna LTP_COMP_NEC_PCS\n",
    "bd_LTP_NEC = bd_LTP_NEC[\n",
    "    [col for col in bd_LTP_NEC.columns if col not in ['ID_RECURSO', 'ID_FERRAMENTA']] +\n",
    "    ['ID_RECURSO', 'ID_FERRAMENTA']\n",
    "]\n",
    "\n",
    "# Mover colunas HOR_REC, HOR_FER, HOR_CAP para o final do DataFrame, depois da coluna LTP_COMP_NEC_PCS\n",
    "bd_LTP_NEC = bd_LTP_NEC[\n",
    "    [col for col in bd_LTP_NEC.columns if col not in ['HOR_REC', 'HOR_FER']] +\n",
    "    ['HOR_REC', 'HOR_FER']\n",
    "]\n",
    "\n",
    "# Excluir coluna HOR_CAP\n",
    "bd_LTP_NEC = drop_colunas(bd_LTP_NEC, ['HOR_CAP'])\n",
    "\n",
    "# criar coluna COMP com valor 'SIM' ou 'NAO' dependendo se o COD_PROD está na tabela bd_estruturas\n",
    "insumos = set(bd_estruturas['cod_insumo'].astype(str).str.strip().str.upper())\n",
    "bd_LTP_NEC['COMP'] = (bd_LTP_NEC['COD_PROD'].astype(str).str.strip().str.upper().isin(insumos).map({True: 'SIM', False: 'NAO'}))\n",
    "\n",
    "print(\"✅ Primeiro Calculo NEC_PCS e Distribuição Capacidade concluídos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039421a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar Data\n",
    "bd_LTP_NEC['MES_REF'] = bd_LTP_NEC['MES_REF'].dt.normalize()\n",
    "\n",
    "# Criar uma lista de MES_REF únicos\n",
    "mes_refs = bd_LTP_NEC['MES_REF'].unique().tolist()\n",
    "\n",
    "# Loop sobre os meses (no momento apenas o primeiro é tratado com cálculo)\n",
    "for mes_ref in mes_refs:\n",
    "    # Filtrar bd_LTP_M1 por mes_ref\n",
    "    if mes_ref == mes_refs[0]:\n",
    "        bd_LTP_M1 = bd_LTP_NEC[bd_LTP_NEC['MES_REF'] == mes_ref].reset_index(drop=True)\n",
    "        bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "        # # *************************# Explosão da Estrutura e necessidade de componentes #**************************\n",
    "        bd_ltp_estrutura_explodida = explodir_estrutura_ltp(bd_estrutura_filtrada, bd_LTP_M1)\n",
    "        bd_nec_comp_expl= calcular_explosao_necessidades(bd_ltp_estrutura_explodida, bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        # *************************# Retorno da NEC_COMP_PCS para a estrutura LTP #********************************\n",
    "        # Agregar campos UNID_PROD, COD_INSUMO, NEC_LIQ_PCS\n",
    "        bd_nec_comp_expl_agreg = (bd_nec_comp_expl.groupby([\"UNID_PROD\", \"COD_INSUMO\"], as_index=False)[\"NEC_LIQ_PCS\"].sum())\n",
    "        \n",
    "        # Faz o merge entre bd_LTP_M1 e bd_nec_comp_expl_agreg\n",
    "        bd_LTP_M1 = bd_LTP_M1.merge(\n",
    "            bd_nec_comp_expl_agreg,\n",
    "            left_on=[\"UNID_FAT\", \"COD_PROD\"],\n",
    "            right_on=[\"UNID_PROD\", \"COD_INSUMO\"],\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_eliminar\")\n",
    "        )\n",
    "\n",
    "        # Atualiza o campo LTP_COMP_NEC_PCS com NEC_LIQ_PCS quando existir\n",
    "        bd_LTP_M1[\"LTP_COMP_NEC_PCS\"] = bd_LTP_M1[\"NEC_LIQ_PCS\"].fillna(bd_LTP_M1[\"LTP_COMP_NEC_PCS\"])\n",
    "\n",
    "        # Remove coluna auxiliar COD_INSUMO se não precisar mais\n",
    "        bd_LTP_M1 = bd_LTP_M1.drop(columns=[\"COD_INSUMO\", \"NEC_LIQ_PCS\", \"UNID_PROD_eliminar\"])\n",
    "        # *********************# Recalculando NEC_PCS e demais campos, após explosão componentes #*******************************\n",
    "        bd_LTP_M1 = calc_nec_pcs_hr(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_HOR_REC, tab_HOR_FER = calcular_distrib_capacidade(bd_LTP_M1, lote_min_flag, multiplo_emb_flag)\n",
    "        bd_LTP_M1, tab_NEC_N_ATEND_PCS, tab_NEC_ESTOURO_PCS, tab_NEC_N_ATEND_PCS_REC_FER = calcular_demais_campos(bd_LTP_M1)\n",
    "        # **************************************# Iniciando Cortes Ferramentas #*************************************************\n",
    "        def aplicar_cortes_recursos(bd_LTP):\n",
    "            \n",
    "            bd_mat_cortes = cria_bd_mat_cortes_REC(bd_LTP)\n",
    "            \n",
    "            # Coluna Total Estoque para otimizar e reduzir tamanho dos próximos calculos que debitam estoque\n",
    "            ET_PCS = bd_LTP_M1['LTP_EST_INI_PCS'] + bd_LTP_M1['LTP_EST_TRANS_PCS'] + bd_LTP_M1['TRIANG_TOT_PCS'] + bd_LTP_M1['ORI_TOT_PCS']\n",
    "            \n",
    "            bd_LTP_M1['ET_PCS'] = np.where(\n",
    "                bd_LTP_M1['NEC_PCS'] == 0,\n",
    "                0,\n",
    "                ET_PCS\n",
    "            )\n",
    "            \n",
    "            # Calculos identificando quantidades PCS não cobertas por estoque e que devem ser cortadas\n",
    "            C_ARR_PCS = (bd_LTP['LTP_CART_ARR_MES_ANT'] - bd_LTP['ET_PCS']).clip(lower=0)\n",
    "            \n",
    "            bd_LTP['C_ARR_PCS'] = np.where(\n",
    "                bd_LTP_M1['NEC_PCS'] == 0,\n",
    "                0,\n",
    "                C_ARR_PCS\n",
    "            )\n",
    "            \n",
    "            C_AT_PCS = ((bd_LTP['LTP_CART_ARR_MES_ANT'] + bd_LTP['LTP_CART_MES_ATUAL']) - bd_LTP['ET_PCS']).clip(lower=0) - bd_LTP['C_ARR_PCS']\n",
    "            \n",
    "            bd_LTP['C_AT_PCS'] = np.where(\n",
    "                bd_LTP_M1['NEC_PCS'] == 0,\n",
    "                0,\n",
    "                C_AT_PCS\n",
    "            )\n",
    "            \n",
    "            PV_PCS  = ((bd_LTP['LTP_CART_ARR_MES_ANT'] + bd_LTP['LTP_CART_MES_ATUAL'] + bd_LTP['LTP_SALDO_PREV_PCS']) - bd_LTP['ET_PCS']).clip(lower=0) - (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'])\n",
    "            \n",
    "            bd_LTP['PV_PCS'] = np.where(\n",
    "                bd_LTP_M1['NEC_PCS'] == 0,\n",
    "                0,\n",
    "                PV_PCS\n",
    "            )\n",
    "            \n",
    "            PV_PROX_PCS = np.where(bd_LTP['MESMA_REG'] == 'NAO', ((bd_LTP['LTP_CART_ARR_MES_ANT'] + bd_LTP['LTP_CART_MES_ATUAL'] + bd_LTP['LTP_SALDO_PREV_PCS'] + bd_LTP['LTP_SALDO_PREV_PROX_MES_PCS']) - bd_LTP['ET_PCS']).clip(lower=0) - (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'] + bd_LTP['PV_PCS']), 0)\n",
    "            \n",
    "            bd_LTP['PV_PROX_PCS'] = np.where(\n",
    "                bd_LTP_M1['NEC_PCS'] == 0,\n",
    "                0,\n",
    "                PV_PROX_PCS\n",
    "            )\n",
    "\n",
    "            bd_LTP['ES_PCS'] = np.where(\n",
    "                bd_LTP['NEC_PCS'] == 0, 0,\n",
    "                np.where(\n",
    "                    bd_LTP['MESMA_REG'] == 'NAO',\n",
    "                    np.maximum(\n",
    "                        (bd_LTP['LTP_CART_ARR_MES_ANT'] + bd_LTP['LTP_CART_MES_ATUAL'] +\n",
    "                        bd_LTP['LTP_SALDO_PREV_PCS'] + bd_LTP['LTP_SALDO_PREV_PROX_MES_PCS'] +\n",
    "                        bd_LTP['LTP_EST_SEG_PCS']) - bd_LTP['ET_PCS'] -\n",
    "                        (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'] +\n",
    "                        bd_LTP['PV_PCS'] + bd_LTP['PV_PROX_PCS']), 0),\n",
    "                    np.maximum(\n",
    "                        (bd_LTP['LTP_CART_ARR_MES_ANT'] + bd_LTP['LTP_CART_MES_ATUAL'] +\n",
    "                        bd_LTP['LTP_SALDO_PREV_PCS'] + bd_LTP['LTP_EST_SEG_PCS']) -\n",
    "                        bd_LTP['ET_PCS'] -\n",
    "                        (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'] +\n",
    "                        bd_LTP['PV_PCS'] + bd_LTP['PV_PROX_PCS']), 0)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            DIF_LM_PCS = bd_LTP['NEC_PCS'] - (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'] + bd_LTP['PV_PCS'] + bd_LTP['PV_PROX_PCS'] + bd_LTP['ES_PCS'])\n",
    "            bd_LTP['DIF_LM_PCS'] = np.where(\n",
    "                bd_LTP['NEC_PCS'] <= 0,\n",
    "                0,\n",
    "                np.where(\n",
    "                    DIF_LM_PCS > bd_LTP['LOTE_MIN'],\n",
    "                    bd_LTP['LOTE_MIN'],\n",
    "                    DIF_LM_PCS\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            DIF_EMB_PCS = bd_LTP['NEC_PCS'] - (bd_LTP['C_ARR_PCS'] + bd_LTP['C_AT_PCS'] + bd_LTP['PV_PCS'] + bd_LTP['PV_PROX_PCS'] + bd_LTP['ES_PCS'] + bd_LTP['DIF_LM_PCS'])\n",
    "            \n",
    "            bd_LTP['DIF_EMB_PCS'] = np.where(\n",
    "                bd_LTP['NEC_PCS'] <= 0,\n",
    "                0,\n",
    "                DIF_EMB_PCS\n",
    "            )\n",
    "            \n",
    "            # Tranformar em HR as colunas calculadas em PCS para cortes\n",
    "            bd_LTP['C_ARR_HR'] = bd_LTP['C_ARR_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['C_AT_HR'] = bd_LTP['C_AT_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['PV_HR'] = bd_LTP['PV_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['PV_PROX_HR'] = bd_LTP['PV_PROX_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['ES_HR'] = bd_LTP['ES_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['DIF_LM_HR'] = bd_LTP['DIF_LM_PCS'] / bd_LTP['PCS_HORA']\n",
    "            bd_LTP['DIF_EMB_HR'] = bd_LTP['DIF_EMB_PCS'] / bd_LTP['PCS_HORA']\n",
    "            \n",
    "            # FIXME: FILTRAR bd_mat_cortes campo ALOC_REC\n",
    "            bd_mat_cortes = bd_mat_cortes[bd_mat_cortes['ALOC_REC'] == '00618'].reset_index(drop=True)\n",
    "            \n",
    "            # Iniciar lopp de cortes por recurso\n",
    "            for i in range(len(bd_mat_cortes)):\n",
    "                aloc_rec = bd_mat_cortes.at[i, 'ALOC_REC']\n",
    "                unid_prod = bd_mat_cortes.at[i, 'UNID_PROD']\n",
    "\n",
    "                # Filtrar bd_LTP para o recurso atual\n",
    "                filtro = (bd_LTP['ALOC_REC'] == aloc_rec) & (bd_LTP['UNID_PROD'] == unid_prod)\n",
    "                bd_LTP_filtrado = bd_LTP.loc[filtro].copy()\n",
    "                \n",
    "                # Somar NEC_PCS com TIPO_PROD = 'PI', para validar se entrar no tratamento de PI ou se segue para tratamento de PA\n",
    "                soma_nec_pcs_pi = bd_LTP_filtrado[bd_LTP_filtrado['TIPO_PROD'] == 'PI']['NEC_PCS'].sum()\n",
    "                \n",
    "                # Verificar se em bd_LTP_filtrado existe TIPO_PROD = PI, e se soma_nec_pcs_pi > 0\n",
    "                if 'PI' in bd_LTP_filtrado['TIPO_PROD'].values and soma_nec_pcs_pi > 0:\n",
    "                    # Listar os COD_PROD e UNID_PROD que sejam TIPO_PROD = \"PI\", eliminando duplicatas, eliminando NEC_PCS = 0\n",
    "                    pi_itens = bd_LTP_filtrado[bd_LTP_filtrado['TIPO_PROD'] == 'PI'][['COD_PROD', 'UNID_PROD', 'PCS_HORA', 'NEC_PCS']].drop_duplicates().reset_index(drop=True)\n",
    "                    pi_itens = pi_itens[pi_itens['NEC_PCS'] > 0].reset_index(drop=True)\n",
    "                    pi_itens['RASTREABILIDADE'] = pi_itens['COD_PROD'] + '|' + pi_itens['UNID_PROD']\n",
    "                    pi_itens['COD_INSUMO'] = pi_itens['COD_PROD']\n",
    "                    # Filtrar COD_PROD e UNID_PROD do pi_itens, nos campos COD_INSUMO e UNID_PROD da bd_ltp_estrutura_explodida\n",
    "                    lista_itens_pa_estr_expl = bd_ltp_estrutura_explodida[\n",
    "                        (bd_ltp_estrutura_explodida[['COD_INSUMO', 'UNID_PROD']].apply(tuple, axis=1).isin(\n",
    "                            pi_itens[['COD_PROD', 'UNID_PROD']].apply(tuple, axis=1)\n",
    "                        ))][['COD_PROD', 'UNID_PROD']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "                    # Filtrar cada COD_PROD e UNID_PROD da lista_itens_pa_estr_expl na bd_LTP_filtrado, na bd_LTP_M1, trazendo as linhas encontradas incoporporando em bd_LTP_filtrado, mantendo o índice original  \n",
    "                    for _, pi in pi_itens.iterrows():\n",
    "                        cod_pi = pi['COD_PROD']\n",
    "                        unid_pi = pi['UNID_PROD']\n",
    "                        rastreabilidade = pi['RASTREABILIDADE']\n",
    "                        pcs_hora_pi = pi['PCS_HORA']\n",
    "                        cod_insumo = pi['COD_INSUMO']\n",
    "\n",
    "                        # PAs originados por esse PI na estrutura\n",
    "                        lista_itens_pa_estr_expl = bd_ltp_estrutura_explodida[\n",
    "                            (bd_ltp_estrutura_explodida['COD_INSUMO'] == cod_pi) &\n",
    "                            (bd_ltp_estrutura_explodida['UNID_PROD'] == unid_pi)\n",
    "                        ][['COD_PROD', 'UNID_PROD']].drop_duplicates()\n",
    "\n",
    "                        for _, pa in lista_itens_pa_estr_expl.iterrows():\n",
    "                            cod_pa = pa['COD_PROD']\n",
    "                            unid_prod_estrutura = pa['UNID_PROD']\n",
    "\n",
    "                            filtro_estrutura = (\n",
    "                                (bd_LTP['COD_PROD'] == cod_pa) &\n",
    "                                (bd_LTP['UNID_PROD'] == unid_prod_estrutura)\n",
    "                            )\n",
    "\n",
    "                            linhas_filtradas_pa_LTP = bd_LTP.loc[filtro_estrutura].copy()\n",
    "\n",
    "                            linhas_filtradas_pa_LTP['RASTREABILIDADE'] = rastreabilidade\n",
    "                            linhas_filtradas_pa_LTP['PCS_HORA_PI'] = pcs_hora_pi\n",
    "                            linhas_filtradas_pa_LTP['COD_INSUMO'] = cod_insumo\n",
    "\n",
    "                            bd_LTP_filtrado = pd.concat(\n",
    "                                [bd_LTP_filtrado, linhas_filtradas_pa_LTP],\n",
    "                                ignore_index=False\n",
    "                            )\n",
    "                            \n",
    "                    # Trazer para bd_LTP_filtrado a coluna FATOR_ESTRUTURAL da bd_estr_fator_estrutural, pelas colunas COD_PROD e UNID_PROD e COD_INSUMO\n",
    "                    bd_LTP_filtrado = bd_LTP_filtrado.merge(\n",
    "                        bd_estr_fator_estrutural[['COD_PROD', 'UNID_PROD', 'COD_INSUMO', 'FATOR_ESTRUTURAL']],\n",
    "                        how='left',\n",
    "                        left_on=['COD_PROD', 'UNID_PROD', 'COD_INSUMO'],\n",
    "                        right_on=['COD_PROD', 'UNID_PROD', 'COD_INSUMO']\n",
    "                    )\n",
    "                    \n",
    "                    # Criar coluna NEC_ATEND_PCS_PI, calculando NEC_ATEND_PCS multiplicando por FATOR_ESTRUTURAL\n",
    "                    bd_LTP_filtrado['NEC_ATEND_PCS_PI'] = bd_LTP_filtrado['NEC_ATEND_PCS'] * bd_LTP_filtrado['FATOR_ESTRUTURAL']\n",
    "                    \n",
    "                    # Criar coluna NEC_ATEND_HR_PI, calculando NEC_ATEND_PCS_PI dividindo por PCS_HORA_PI\n",
    "                    bd_LTP_filtrado['NEC_ATEND_HR_PI'] = bd_LTP_filtrado['NEC_ATEND_PCS_PI'] / bd_LTP_filtrado['PCS_HORA_PI']\n",
    "                    \n",
    "                    # ----------------------------------------- Realizar Rateio ---------------------------------------------\n",
    "                    # Distribuir NEC_ATEND_HR_PI nas colunas C_ARR_HR, C_AT_HR, PV_HR, PV_PROX_HR, ES_HR, DIF_LM_HR, DIF_EMB_HR proporcionalmente\n",
    "                    cols_rateio = ['C_ARR_HR','C_AT_HR','PV_HR','PV_PROX_HR','ES_HR','DIF_LM_HR','DIF_EMB_HR']\n",
    "\n",
    "                    # 1. Copiar base original\n",
    "                    base = bd_LTP_filtrado[cols_rateio].copy()\n",
    "\n",
    "                    # 2. Calcular base de rateio\n",
    "                    base_sum = base.sum(axis=1)\n",
    "\n",
    "                    # 3. Evitar divisão por zero\n",
    "                    fator = bd_LTP_filtrado['NEC_ATEND_HR_PI'].where(base_sum > 0, 0)\n",
    "\n",
    "                    # 4. Rateio e sobreescrever as colunas originais com os valores rateados\n",
    "                    bd_LTP_filtrado[cols_rateio] = base.div(base_sum, axis=0).mul(fator, axis=0).fillna(0)\n",
    "                    # --------------------------------------- Fim Realizar Rateio -------------------------------------------\n",
    "                    \n",
    "                    # Loop sob a bd_mat_cortes para identificar o corte CORTE_HR e aplicar na bd_LTP_filtrado, conforme matriz de cortes e recursos, filtros de mesma região e recurso, e coluna de referencia da matriz de cortes, para identificar os produtos que devem ser cortados, e as horas a serem cortadas\n",
    "                    for j in range(len(bd_mat_cortes)):\n",
    "                        aloc_rec_corte = bd_mat_cortes.at[j, 'ALOC_REC']\n",
    "                        unid_prod_corte = bd_mat_cortes.at[j, 'UNID_PROD']\n",
    "                        corte_hr = bd_mat_cortes.at[j, 'CORTE_HR']\n",
    "                        coluna_ref_corte = bd_mat_cortes.at[j, 'COLUNA_REF']\n",
    "\n",
    "                        # Filtrar bd_LTP_filtrado para o recurso e unidade de produto do corte\n",
    "                        filtro_corte = (bd_LTP_filtrado['ALOC_REC'] == aloc_rec_corte) & (bd_LTP_filtrado['UNID_PROD'] == unid_prod_corte)\n",
    "                        bd_LTP_filtrado_corte = bd_LTP_filtrado.loc[filtro_corte].copy()\n",
    "\n",
    "                        # Aplicar o corte_hr na coluna_ref_corte da bd_LTP_filtrado_corte, reduzindo o valor da coluna_ref_corte pelo corte_hr, sem reduzir abaixo de zero\n",
    "                        bd_LTP_filtrado.loc[filtro_corte, coluna_ref_corte] = (bd_LTP_filtrado.loc[filtro_corte, coluna_ref_corte] - corte_hr).clip(lower=0)\n",
    "                    \n",
    "                    \n",
    "                    # Função que cria a matriz de cortes, e sequencia de cortes que deve ser respeitada\n",
    "                    bd_matriz_cortes_rec = matriz_cortes_horas(bd_LTP_filtrado)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    '''\n",
    "                        Filtrar mesma região matriz e coluna de analise no BD_FILTRADO\n",
    "                        IR na coluna especifica e calcular partipação no total, de cada produto, na coluna foco da matriz de cortes\n",
    "                        \n",
    "                    '''\n",
    "                    \n",
    "                else:\n",
    "                    # Aplicar cortes pois no filtro máquina temos somente PAs\n",
    "                    pass\n",
    "            return  bd_LTP, bd_mat_cortes, bd_LTP_filtrado, pi_itens, linhas_filtradas_pa_LTP, bd_estr_fator_estrutural, bd_matriz_cortes_rec\n",
    "\n",
    "    bd_LTP_M1, bd_mat_cortes, bd_LTP_filtrado, pi_itens, linhas_filtradas_pa_LTP, bd_estr_fator_estrutural, bd_matriz_cortes_rec = aplicar_cortes_recursos(bd_LTP_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af8c5c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNID_PROD</th>\n",
       "      <th>ALOC_REC</th>\n",
       "      <th>HOR_REC</th>\n",
       "      <th>NEC_ESTOURO_HR_REC</th>\n",
       "      <th>NEC_ATEND_HR</th>\n",
       "      <th>%_OCUP_REC</th>\n",
       "      <th>CORTE_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matriz</td>\n",
       "      <td>00618</td>\n",
       "      <td>511.52</td>\n",
       "      <td>289.19</td>\n",
       "      <td>511.52</td>\n",
       "      <td>156.54</td>\n",
       "      <td>289.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UNID_PROD ALOC_REC  HOR_REC  NEC_ESTOURO_HR_REC  NEC_ATEND_HR  %_OCUP_REC  CORTE_HR\n",
       "0    Matriz    00618   511.52              289.19        511.52      156.54    289.19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_mat_cortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917edc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_LTP_M1.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_mat_cortes.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_estrutura_filtrada.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_nec_comp_expl.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_LTP_filtrado.xlsx\n",
      "✅ Exportado: c:\\Users\\carlo\\OneDrive\\BC\\03. Projetos Bedin\\01. Krona\\LTP\\02_OUTPUT\\bd_estr_fator_estrutural.xlsx\n",
      "Tempo total de processamento: 28 min 47.2 s\n",
      "🎯 Processo concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Exportar para Excel\n",
    "\n",
    "# Listar os dataframes e seus nomes correspondentes\n",
    "dataframes_para_exportar = {\n",
    "    \"bd_LTP_M1\": bd_LTP_M1,\n",
    "    \"bd_mat_cortes\": bd_mat_cortes,\n",
    "    \"bd_estrutura_filtrada\": bd_estrutura_filtrada,\n",
    "    \"bd_nec_comp_expl\": bd_nec_comp_expl,\n",
    "    \"bd_LTP_filtrado\": bd_LTP_filtrado,\n",
    "    \"bd_estr_fator_estrutural\": bd_estr_fator_estrutural\n",
    "}\n",
    "\n",
    "for nome in dataframes_para_exportar:\n",
    "    df = globals()[nome]   # pega o objeto DataFrame pelo nome da variável\n",
    "    caminho_arquivo = pasta_output / f\"{nome}.xlsx\"\n",
    "    df.to_excel(caminho_arquivo, index=False)\n",
    "    print(f\"✅ Exportado: {caminho_arquivo}\")\n",
    "    \n",
    "timer.finalizar()\n",
    "print(\"🎯 Processo concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
